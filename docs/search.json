[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Best Practices for Using eBird Data",
    "section": "",
    "text": "Matthew Strimas-Mackey, Wesley M. Hochachka, Viviana Ruiz-Gutierrez, Orin J. Robinson, Eliot T. Miller, Tom Auer, Steve Kelling, Daniel Fink, Alison Johnston\nVersion 2.0\n\nWelcome\nBest Practices for Using eBird Data is a supplement to Analytical guidelines to increase the value of community science data: An example using eBird data to estimate species distributions (Johnston et al. 2021). This paper describes the challenges associated with making inferences from biological citizen science data and proposes a set of best practices for making reliable estimates of species distributions from these data. Throughout, the paper uses eBird, the world’s largest biological citizen science project, as a case study to illustrate the best practices. This guide acts as a supplement to the paper, showing readers how to implement these best practices within R using real data from eBird. After completing this guide, readers should be able to process eBird data to prepare them for robust analyses, train models to estimate encounter rate and relative abundance, and assess the performance of these models. Readers should be comfortable with the R programming language, and read the Background Knowledge and Setup sections of the introduction, before diving into this guide.\nTo submit fixes or suggest additions and improvements to this guide, please file an issue on GitHub.\nThe code for Version 1.0 of this guide is available on GitHub.\nPlease cite this guide as:\n\nStrimas-Mackey, M., W.M. Hochachka, V. Ruiz-Gutierrez, O.J. Robinson, E.T. Miller, T. Auer, S. Kelling, D. Fink, A. Johnston. 2023. Best Practices for Using eBird Data. Version 2.0. https://ebird.github.io/ebird-best-practices/. Cornell Lab of Ornithology, Ithaca, New York. https://doi.org/10.5281/zenodo.3620739\n\n\n\n\n\nJohnston, Alison, Wesley M. Hochachka, Matthew E. Strimas-Mackey, Viviana Ruiz Gutierrez, Orin J. Robinson, Eliot T. Miller, Tom Auer, Steve T. Kelling, and Daniel Fink. 2021. “Analytical Guidelines to Increase the Value of Community Science Data: An Example Using eBird Data to Estimate Species Distributions.” Edited by Yoan Fourcade. Diversity and Distributions 27 (7): 1265–77. https://doi.org/10.1111/ddi.13271."
  },
  {
    "objectID": "intro.html#sec-intro-intro",
    "href": "intro.html#sec-intro-intro",
    "title": "1  Introduction and Setup",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nCitizen science data are increasingly making important contributions to ecological research and conservation. One of the most common forms of citizen science data is derived from members of the public recording species observations. eBird (Sullivan et al. 2014) is the largest of these biological citizen science programs. The eBird database contains well over one billion bird observations from every country in the world, with observations of nearly every bird species on Earth. The eBird database is valuable to researchers across the globe, due to its year-round, broad spatial coverage, high volumes of open access data, and applications to many ecological questions. These data have been widely used in scientific research to study phenology, species distributions, population trends, evolution, behavior, global change, and conservation. However, robust inference with eBird data requires careful processing of the data to address the challenges associated with citizen science datasets. This guide, and the associated paper (Johnston et al. 2021), outlines a set of best practices for addressing these challenges and making reliable estimates of species distributions from eBird data.\nThe next chapter provides an introduction to eBird data, then demonstrates how to access and prepare the data for modeling. The following chapter covers preparing environmental variables to be used as model predictors. The remaining two chapters demonstrate different species distribution models that can be fit using these data: encounter rate models and relative abundance models. Although the examples used throughout this guide focus on eBird data, in many cases the techniques they illustrate also apply to similar citizen science datasets."
  },
  {
    "objectID": "intro.html#sec-intro-background",
    "href": "intro.html#sec-intro-background",
    "title": "1  Introduction and Setup",
    "section": "1.2 Background knowledge",
    "text": "1.2 Background knowledge\nTo understand the code examples used throughout this guide, some knowledge of the programming language R is required. If you don’t meet this requirement, or begin to feel lost trying to understand the code used in this guide, we suggest consulting one of the excellent free resources available online for learning R. For those with little or no prior programming experience, Hands-On Programming with R is an excellent introduction. For those with some familiarity with the basics of R that want to take their skills to the next level, we suggest R for Data Science as the best resource for learning how to work with data within R.\n\n1.2.1 Tidyverse\nThroughout this guide, we use packages from the Tidyverse, an opinionated collection of R packages designed for data science. Packages such as ggplot2, for data visualization, and dplyr, for data manipulation, are two of the most well known Tidyverse packages; however, there are many more. In the following chapters, we often use Tidyverse functions without explanation. If you encounter a function you’re unfamiliar with, consult the documentation for help (e.g. ?mutate to see help for the dplyr function mutate()). More generally, the free online guide R for Data Science by Hadley Wickham is the best introduction to working with data in R using the Tidyverse."
  },
  {
    "objectID": "intro.html#sec-intro-background-pipe",
    "href": "intro.html#sec-intro-background-pipe",
    "title": "1  Introduction and Setup",
    "section": "1.3 The pipe operator",
    "text": "1.3 The pipe operator\nThe one specific piece of syntax we cover here, because it is ubiquitous throughout this guide and unfamiliar to some, is the pipe operator |&gt;. The pipe operator takes the expression to the left of it and “pipes” it into the first argument of the expression on the right, i.e. one can replace f(x) with x |&gt; f(). The pipe makes code significantly more readable by avoiding nested function calls, reducing the need for intermediate variables, and making sequential operations read left-to-right. For example, to add a new variable to a data frame, then summarize using a grouping variable, the following are equivalent:\n\nlibrary(dplyr)\n\n# pipes\nmtcars |&gt; \n  mutate(wt_kg = 454 * wt) |&gt; \n  group_by(cyl) |&gt; \n  summarize(wt_kg = mean(wt_kg))\n#&gt; # A tibble: 3 × 2\n#&gt;     cyl wt_kg\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     4 1038.\n#&gt; 2     6 1415.\n#&gt; 3     8 1816.\n\n# intermediate variables\nmtcars_kg &lt;- mutate(mtcars, wt_kg = 454 * wt)\nmtcars_grouped &lt;- group_by(mtcars_kg, cyl)\nsummarize(mtcars_grouped, wt_kg = mean(wt_kg))\n#&gt; # A tibble: 3 × 2\n#&gt;     cyl wt_kg\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     4 1038.\n#&gt; 2     6 1415.\n#&gt; 3     8 1816.\n\n# nested function calls\nsummarize(\n  group_by(\n    mutate(mtcars, wt_kg = 454 * wt),\n    cyl\n  ),\n  wt_kg = mean(wt_kg)\n)\n#&gt; # A tibble: 3 × 2\n#&gt;     cyl wt_kg\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     4 1038.\n#&gt; 2     6 1415.\n#&gt; 3     8 1816.\n\nOnce you become familiar with the pipe operator, we believe you’ll find the the above example using the pipe the easiest of the three to read and interpret.\n\n\n\n\n\n\nExercise\n\n\n\nRewrite the following code using pipes:\n\nset.seed(1)\nround(log(runif(10, min = 0.5)), 1)\n#&gt;  [1] -0.5 -0.4 -0.2  0.0 -0.5 -0.1  0.0 -0.2 -0.2 -0.6\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nset.seed(1)\nrunif(10, min = 0.5) |&gt; \n  log() |&gt; \n  round(digits = 1)\n#&gt;  [1] -0.5 -0.4 -0.2  0.0 -0.5 -0.1  0.0 -0.2 -0.2 -0.6\n\n\n\n\n\n1.3.1 Working with spatial data in R\nSome familiarity with the main spatial R packages sf and terra will be necessary to following along with this guide. The free online book Geocomputation with R is a good resource on working with spatial data in R."
  },
  {
    "objectID": "intro.html#sec-intro-setup",
    "href": "intro.html#sec-intro-setup",
    "title": "1  Introduction and Setup",
    "section": "1.4 Setup",
    "text": "1.4 Setup\n\n1.4.1 Data package\nThe next two chapters of this guide focus on obtaining and preparing eBird data and environmental variables for the modeling that will occur in the remaining chapters. These steps can be time consuming and laborious. If you’d like to skip straight to the analysis, download this package of prepared data. Unzipping this file should produce two directories: data/ and data-raw/. Move both these directories so they are subdirectory of your RStudio project folder. This will allow you to jump right in to the modeling and ensure that you’re using exactly the same data as was used when creating this guide. This is a good option if you don’t have a fast enough internet connection to download the eBird data.\n\n\n1.4.2 Software\nThe examples throughout this website use the programming language R (R Core Team 2023) to work with eBird data. If you don’t have R installed, download it now, if you already have R, there’s a good chance you have an outdated version, so update it to the latest version now. R is updated regularly, and it is important that you have the most recent version of R to avoid headaches when installing packages. We suggest checking every couple months to see if a new version has been released.\nWe strongly encourage R users to use RStudio. RStudio is not required to follow along with this guide; however, it will make your R experience significantly better. If you don’t have RStudio, download it now, if you already have it, update it because new versions with useful additional features are regularly released.\nDue to the large size of the eBird dataset, working with it requires the Unix command-line utility AWK. You won’t need to use AWK directly, since the R package auk does this hard work for you, but you do need AWK to be installed on your computer. Linux and Mac users should already have AWK installed on their machines; however, Windows user will need to install Cygwin to gain access to AWK. Cygwin is free software that allows Windows users to use Unix tools. Cygwin should be installed in the default location (C:/cygwin/bin/gawk.exe or C:/cygwin64/bin/gawk.exe) in order for everything to work correctly. Note: there’s no need to do anything at the “Select Utilities” screen, AWK will be installed by default.\n\n\n1.4.3 R packages\nThe examples in this guide use a variety of R packages for accessing eBird data, working with spatial data, data processing and manipulation, and model training. To install all the packages necessary to work through this guide, run the following code:\n\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  install.packages(\"remotes\")\n}\nremotes::install_github(\"ebird/ebird-best-practices\")\n\nNote that several of the spatial packages require dependencies. If installing these packages fails, consult the instructions for installing dependencies on the sf package website. Finally, ensure all R packages are updated to their most recent versions by clicking on the Update button on the Packages tab in RStudio.\n\n\n1.4.4 eBird data access\nAccess to the eBird database is provided via the eBird Basic Dataset (EBD) as tab-separated text files. To access the EBD, begin by creating an eBird account and signing in. Then visit the eBird Data Access page and fill out the data access request form. eBird data access is free for most uses; however, you will need to request access in order to download the EBD. Filling out the access request form allows eBird to keep track of the number of people using the data and obtain information on the applications for which the data are used.\nOnce you’ve been granted access to the EBD, you will be able to download either the entire eBird dataset or subsets for specific species, regions, or time periods. This is covered in more detail in the next chapter.\n\n\n1.4.5 GIS data\nThroughout this guide, we’ll be producing maps of species distributions. To provide context for these distributions, we’ll need GIS data for political boundaries. Natural Earth is the best source for a range of tightly integrated vector and raster GIS data for producing professional cartographic maps. The R package, rnaturalearth provides a convenient method for accessing these data from within R.\nThe data package mentioned in Section 1.4.1 contains a GeoPackage with all the necessary GIS data. However, for reference, the following code was used to generate the GIS dataset. Running this code will create a GeoPackage containing the necessary spatial layers in data/gis-data.gpkg.\n\nlibrary(dplyr)\nlibrary(rnaturalearth)\nlibrary(sf)\n\n# file to save spatial data\ngpkg_file &lt;- \"data/gis-data.gpkg\"\ndir.create(dirname(gpkg_file), showWarnings = FALSE, recursive = TRUE)\n\n# political boundaries\n# land border with lakes removed\nne_land &lt;- ne_download(scale = 50, category = \"cultural\",\n                       type = \"admin_0_countries_lakes\",\n                       returnclass = \"sf\") |&gt;\n  filter(CONTINENT %in% c(\"North America\", \"South America\")) |&gt;\n  st_set_precision(1e6) |&gt;\n  st_union()\n# country boundaries\nne_countries &lt;- ne_download(scale = 50, category = \"cultural\",\n                       type = \"admin_0_countries_lakes\",\n                       returnclass = \"sf\") |&gt;\n  select(country = ADMIN, country_code = ISO_A2)\n# state boundaries for united states\nne_states &lt;- ne_download(scale = 50, category = \"cultural\",\n                       type = \"admin_1_states_provinces\",\n                       returnclass = \"sf\") |&gt; \n  filter(iso_a2 == \"US\") |&gt; \n  select(state = name, state_code = iso_3166_2)\n# country lines\n# downloaded globally then filtered to north america with st_intersect()\nne_country_lines &lt;- ne_download(scale = 50, category = \"cultural\",\n                                type = \"admin_0_boundary_lines_land\",\n                                returnclass = \"sf\") |&gt; \n  st_geometry()\nlines_on_land &lt;- st_intersects(ne_country_lines, ne_land, sparse = FALSE) |&gt;\n  as.logical()\nne_country_lines &lt;- ne_country_lines[lines_on_land]\n# states, north america\nne_state_lines &lt;- ne_download(scale = 50, category = \"cultural\",\n                              type = \"admin_1_states_provinces_lines\",\n                              returnclass = \"sf\") |&gt;\n  filter(ADM0_A3 %in% c(\"USA\", \"CAN\")) |&gt;\n  mutate(iso_a2 = recode(ADM0_A3, USA = \"US\", CAN = \"CAN\")) |&gt; \n  select(country = ADM0_NAME, country_code = iso_a2)\n\n# save all layers to a geopackage\nunlink(gpkg_file)\nwrite_sf(ne_land, gpkg_file, \"ne_land\")\nwrite_sf(ne_countries, gpkg_file, \"ne_countries\")\nwrite_sf(ne_states, gpkg_file, \"ne_states\")\nwrite_sf(ne_country_lines, gpkg_file, \"ne_country_lines\")\nwrite_sf(ne_state_lines, gpkg_file, \"ne_state_lines\")"
  },
  {
    "objectID": "intro.html#sec-intro-info",
    "href": "intro.html#sec-intro-info",
    "title": "1  Introduction and Setup",
    "section": "1.5 Session info",
    "text": "1.5 Session info\nThis guide was compiled using the latest version of R and all R packages at the time of compilation. If you encounter errors while running code in this guide it is likely that they are being caused by differences in package versions between your R session and the one used to compile this book. To help diagnose this issue, we use devtools::session_info() to list the versions of all R packages used to compile this guide.\n\ndevtools::session_info()\n#&gt; ─ Session info ───────────────────────────────────────────────────────────────\n#&gt;  setting  value\n#&gt;  version  R version 4.3.2 (2023-10-31)\n#&gt;  os       macOS Sonoma 14.4\n#&gt;  system   x86_64, darwin20\n#&gt;  ui       X11\n#&gt;  language (EN)\n#&gt;  collate  en_US.UTF-8\n#&gt;  ctype    en_US.UTF-8\n#&gt;  tz       America/Los_Angeles\n#&gt;  date     2024-03-22\n#&gt;  pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#&gt; \n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package          * version  date (UTC) lib source\n#&gt;  auk              * 0.7.0    2023-11-14 [1] CRAN (R 4.3.0)\n#&gt;  cachem             1.0.8    2023-05-01 [1] CRAN (R 4.3.0)\n#&gt;  class              7.3-22   2023-05-03 [2] CRAN (R 4.3.2)\n#&gt;  classInt           0.4-10   2023-09-05 [1] CRAN (R 4.3.0)\n#&gt;  cli                3.6.2    2023-12-11 [1] CRAN (R 4.3.0)\n#&gt;  codetools          0.2-19   2023-02-01 [2] CRAN (R 4.3.2)\n#&gt;  colorspace         2.1-0    2023-01-23 [1] CRAN (R 4.3.0)\n#&gt;  data.table         1.15.0   2024-01-30 [1] CRAN (R 4.3.2)\n#&gt;  DBI                1.2.2    2024-02-16 [1] CRAN (R 4.3.2)\n#&gt;  devtools           2.4.5    2022-10-11 [1] CRAN (R 4.3.0)\n#&gt;  digest             0.6.34   2024-01-11 [1] CRAN (R 4.3.0)\n#&gt;  dotCall64          1.1-1    2023-11-28 [2] CRAN (R 4.3.0)\n#&gt;  dplyr            * 1.1.4    2023-11-17 [1] CRAN (R 4.3.0)\n#&gt;  e1071              1.7-14   2023-12-06 [1] CRAN (R 4.3.0)\n#&gt;  ebirdst          * 3.2022.3 2024-03-05 [1] CRAN (R 4.3.2)\n#&gt;  ellipsis           0.3.2    2021-04-29 [1] CRAN (R 4.3.0)\n#&gt;  evaluate           0.23     2023-11-01 [1] CRAN (R 4.3.0)\n#&gt;  exactextractr    * 0.10.0   2023-09-20 [2] CRAN (R 4.3.0)\n#&gt;  fansi              1.0.6    2023-12-08 [1] CRAN (R 4.3.0)\n#&gt;  fastmap            1.1.1    2023-02-24 [1] CRAN (R 4.3.0)\n#&gt;  fields           * 15.2     2023-08-17 [2] CRAN (R 4.3.0)\n#&gt;  fs                 1.6.3    2023-07-20 [1] CRAN (R 4.3.0)\n#&gt;  generics           0.1.3    2022-07-05 [1] CRAN (R 4.3.0)\n#&gt;  ggplot2          * 3.5.0    2024-02-23 [1] CRAN (R 4.3.2)\n#&gt;  glue               1.7.0    2024-01-09 [1] CRAN (R 4.3.0)\n#&gt;  gridExtra        * 2.3      2017-09-09 [2] CRAN (R 4.3.0)\n#&gt;  gtable             0.3.4    2023-08-21 [1] CRAN (R 4.3.0)\n#&gt;  hms                1.1.3    2023-03-21 [1] CRAN (R 4.3.0)\n#&gt;  htmltools          0.5.7    2023-11-03 [1] CRAN (R 4.3.0)\n#&gt;  htmlwidgets        1.6.4    2023-12-06 [1] CRAN (R 4.3.0)\n#&gt;  httpuv             1.6.14   2024-01-26 [1] CRAN (R 4.3.2)\n#&gt;  jsonlite           1.8.8    2023-12-04 [1] CRAN (R 4.3.0)\n#&gt;  KernSmooth         2.23-22  2023-07-10 [2] CRAN (R 4.3.2)\n#&gt;  knitr              1.45     2023-10-30 [1] CRAN (R 4.3.0)\n#&gt;  landscapemetrics * 2.1.1    2024-01-09 [2] CRAN (R 4.3.0)\n#&gt;  later              1.3.2    2023-12-06 [1] CRAN (R 4.3.0)\n#&gt;  lattice            0.21-9   2023-10-01 [2] CRAN (R 4.3.2)\n#&gt;  lifecycle          1.0.4    2023-11-07 [1] CRAN (R 4.3.0)\n#&gt;  lobstr             1.1.2    2022-06-22 [2] CRAN (R 4.3.0)\n#&gt;  lubridate        * 1.9.3    2023-09-27 [1] CRAN (R 4.3.0)\n#&gt;  magrittr           2.0.3    2022-03-30 [1] CRAN (R 4.3.0)\n#&gt;  maps               3.4.2    2023-12-15 [2] CRAN (R 4.3.0)\n#&gt;  Matrix             1.6-1.1  2023-09-18 [2] CRAN (R 4.3.2)\n#&gt;  mccf1            * 1.1      2019-11-11 [2] CRAN (R 4.3.0)\n#&gt;  memoise            2.0.1    2021-11-26 [1] CRAN (R 4.3.0)\n#&gt;  mgcv             * 1.9-0    2023-07-11 [2] CRAN (R 4.3.2)\n#&gt;  mime               0.12     2021-09-28 [1] CRAN (R 4.3.0)\n#&gt;  miniUI             0.1.1.1  2018-05-18 [1] CRAN (R 4.3.0)\n#&gt;  munsell            0.5.0    2018-06-12 [1] CRAN (R 4.3.0)\n#&gt;  nlme             * 3.1-163  2023-08-09 [2] CRAN (R 4.3.2)\n#&gt;  pillar             1.9.0    2023-03-22 [1] CRAN (R 4.3.0)\n#&gt;  pkgbuild           1.4.3    2023-12-10 [1] CRAN (R 4.3.0)\n#&gt;  pkgconfig          2.0.3    2019-09-22 [1] CRAN (R 4.3.0)\n#&gt;  pkgload            1.3.4    2024-01-16 [1] CRAN (R 4.3.0)\n#&gt;  precrec          * 0.14.4   2023-10-11 [2] CRAN (R 4.3.0)\n#&gt;  PresenceAbsence  * 1.1.11   2023-01-07 [2] CRAN (R 4.3.0)\n#&gt;  profvis            0.3.8    2023-05-02 [1] CRAN (R 4.3.0)\n#&gt;  promises           1.2.1    2023-08-10 [1] CRAN (R 4.3.0)\n#&gt;  proxy              0.4-27   2022-06-09 [1] CRAN (R 4.3.0)\n#&gt;  pryr               0.1.6    2023-01-17 [2] CRAN (R 4.3.0)\n#&gt;  purrr              1.0.2    2023-08-10 [1] CRAN (R 4.3.0)\n#&gt;  R6                 2.5.1    2021-08-19 [1] CRAN (R 4.3.0)\n#&gt;  ranger           * 0.16.0   2023-11-12 [1] CRAN (R 4.3.0)\n#&gt;  raster             3.6-26   2023-10-14 [2] CRAN (R 4.3.0)\n#&gt;  Rcpp               1.0.12   2024-01-09 [1] CRAN (R 4.3.0)\n#&gt;  readr            * 2.1.5    2024-01-10 [1] CRAN (R 4.3.0)\n#&gt;  remotes            2.4.2.1  2023-07-18 [1] CRAN (R 4.3.0)\n#&gt;  rlang              1.1.3    2024-01-10 [1] CRAN (R 4.3.0)\n#&gt;  rmarkdown          2.25     2023-09-18 [1] CRAN (R 4.3.0)\n#&gt;  rstudioapi         0.15.0   2023-07-07 [1] CRAN (R 4.3.0)\n#&gt;  scales             1.3.0    2023-11-28 [1] CRAN (R 4.3.0)\n#&gt;  scam             * 1.2-14   2023-04-14 [2] CRAN (R 4.3.0)\n#&gt;  sessioninfo        1.2.2    2021-12-06 [1] CRAN (R 4.3.0)\n#&gt;  sf               * 1.0-15   2023-12-18 [1] CRAN (R 4.3.0)\n#&gt;  shiny              1.8.0    2023-11-17 [1] CRAN (R 4.3.0)\n#&gt;  sp                 2.1-2    2023-11-26 [2] CRAN (R 4.3.0)\n#&gt;  spam             * 2.10-0   2023-10-23 [2] CRAN (R 4.3.0)\n#&gt;  stringi            1.8.3    2023-12-11 [1] CRAN (R 4.3.0)\n#&gt;  stringr          * 1.5.1    2023-11-14 [1] CRAN (R 4.3.0)\n#&gt;  terra            * 1.7-71   2024-01-31 [1] CRAN (R 4.3.2)\n#&gt;  tibble             3.2.1    2023-03-20 [1] CRAN (R 4.3.0)\n#&gt;  tidyr            * 1.3.1    2024-01-24 [1] CRAN (R 4.3.2)\n#&gt;  tidyselect         1.2.0    2022-10-10 [1] CRAN (R 4.3.0)\n#&gt;  timechange         0.3.0    2024-01-18 [1] CRAN (R 4.3.0)\n#&gt;  tzdb               0.4.0    2023-05-12 [1] CRAN (R 4.3.0)\n#&gt;  units            * 0.8-5    2023-11-28 [1] CRAN (R 4.3.0)\n#&gt;  urlchecker         1.0.1    2021-11-30 [1] CRAN (R 4.3.0)\n#&gt;  usethis            2.2.3    2024-02-19 [1] CRAN (R 4.3.2)\n#&gt;  utf8               1.2.4    2023-10-22 [1] CRAN (R 4.3.0)\n#&gt;  vctrs              0.6.5    2023-12-01 [1] CRAN (R 4.3.0)\n#&gt;  viridis          * 0.6.4    2023-07-22 [2] CRAN (R 4.3.0)\n#&gt;  viridisLite      * 0.4.2    2023-05-02 [1] CRAN (R 4.3.0)\n#&gt;  withr              3.0.0    2024-01-16 [1] CRAN (R 4.3.0)\n#&gt;  xfun               0.42     2024-02-08 [1] CRAN (R 4.3.2)\n#&gt;  xtable             1.8-4    2019-04-21 [1] CRAN (R 4.3.0)\n#&gt;  yaml               2.3.8    2023-12-11 [1] CRAN (R 4.3.0)\n#&gt; \n#&gt;  [1] /Users/mes335/Library/R/x86_64/4.3/library\n#&gt;  [2] /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\nJohnston, Alison, Wesley M. Hochachka, Matthew E. Strimas-Mackey, Viviana Ruiz Gutierrez, Orin J. Robinson, Eliot T. Miller, Tom Auer, Steve T. Kelling, and Daniel Fink. 2021. “Analytical Guidelines to Increase the Value of Community Science Data: An Example Using eBird Data to Estimate Species Distributions.” Edited by Yoan Fourcade. Diversity and Distributions 27 (7): 1265–77. https://doi.org/10.1111/ddi.13271.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Manual. Vienna, Austria: R Foundation for Statistical Computing.\n\n\nSullivan, Brian L., Jocelyn L. Aycrigg, Jessie H. Barry, Rick E. Bonney, Nicholas Bruns, Caren B. Cooper, Theo Damoulas, et al. 2014. “The eBird Enterprise: An Integrated Approach to Development and Application of Citizen Science.” Biological Conservation 169 (January): 31–40. https://doi.org/10.1016/j.biocon.2013.11.003."
  },
  {
    "objectID": "ebird.html#sec-ebird-intro",
    "href": "ebird.html#sec-ebird-intro",
    "title": "2  eBird Data",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\neBird data are collected and organized around the concept of a checklist, representing observations from a single birding event, such as a 1 km walk through a park or 15 minutes observing bird feeders in your backyard. Each checklist contains a list of species observed, counts of the number of individuals seen of each species, the location and time of the observations, information on the type of survey performed, and measures of the effort expended while collecting these data. The following image depicts a typical eBird checklist as viewed on the eBird website:\n\nThere are three key characteristics that distinguish eBird from many other citizen science projects and facilitate robust ecological analyses. First, observers specify the survey protocol they used, whether it’s traveling, stationary, incidental (i.e., if the observations were collected when birding was not the primary activity), or one of the other protocols. These protocols are designed to be flexible, allowing observers to collect data during their typical birding outings. Second, in addition to typical information on when and where the observations were made, observers record effort information specifying how long they searched, how far they traveled, and the total number of observers in their party. Collecting this data facilitates robust analyses by allowing researchers to account for variation in the observation process (La Sorte et al. 2018; Kelling et al. 2018). Finally, observers are asked to indicate whether they are reporting all the birds they were able to detect and identify. Checklists with all species reported, known as complete checklists, enable researchers to infer counts of zero individuals for the species that were not reported. If checklists are not complete, it’s not possible to ascertain whether the absence of a species on a list was a non-detection or the result of a participant not recording the species.\nCitizen science projects occur on a spectrum from those with predefined sampling structures that resemble more traditional survey designs (such as the Breeding Bird Survey in the United States), to those that are unstructured and collect observations opportunistically (such as iNaturalist). We refer to eBird as a semi-structured project (Kelling et al. 2018), having flexible, easy to follow protocols that attract many participants, but also collecting data on the observation process and allowing non-detections to be inferred on complete checklists.\nIn this chapter, we’ll highlight some of the challenges associated with using eBird data. Then we’ll demonstrate how to download eBird data for a given region and species. Next, we’ll show how to import the data into R, apply filters to it, and use complete checklists to produce detection/non-detection data suitable for modeling species distribution and abundance. Finally, we’ll perform some pre-processing steps required to ensure proper analysis of the data.\n\n\n\n\n\n\nTip\n\n\n\nWe use the terms detection and non-detection rather than the more common terms presence and absence throughout this guide to reflect the fact that an inferred count of zero does not necessarily mean that a species is absent, only that it was not detected on the checklist in question."
  },
  {
    "objectID": "ebird.html#sec-ebird-challenges",
    "href": "ebird.html#sec-ebird-challenges",
    "title": "2  eBird Data",
    "section": "2.2 Challenges associated with eBird data",
    "text": "2.2 Challenges associated with eBird data\nDespite the strengths of eBird data, species observations collected through citizen science projects present a number of challenges that are not found in conventional scientific data. The following are some of the primary challenges associated these data; challenges that will be addressed throughout this guide:\n\nTaxonomic bias: participants often have preferences for certain species, which may lead to preferential recording of some species over others (Greenwood 2007; Tulloch and Szabo 2012). Restricting analyses to complete checklists largely mitigates this issue.\nSpatial bias: most participants in citizen science surveys sample near their homes (Luck et al. 2004), in easily accessible areas such as roadsides (Kadmon, Farber, and Danin 2004), or in areas and habitats of known high biodiversity (Prendergast et al. 1993). A simple method to reduce the spatial bias that we describe is to create an equal area grid over the region of interest, and sample a given number of checklists from within each grid cell.\nTemporal bias: participants preferentially sample when they are available, such as weekends (Courter et al. 2013), and at times of year when they expect to observe more birds, notably in the United States there is a large increase in eBird submissions during spring migration (Sullivan et al. 2014). Furthermore, eBird has steadily increased in popularity over time, leading a strong bias towards more data in recent years. To address the weekend bias, we recommend using a temporal scale of a week or multiple weeks for most analyses. Temporal biases at longer time scales can be addressed by subsampling the data to produce a more even temporal distribution.\nClass imbalance: bird species that are rare or hard to detect may have data with high class imbalance, with many more checklists with non-detections than detections. For these species, a distribution model predicting that the species is absent everywhere will have high accuracy, but no ecological value. We’ll follow the methods for addressing class imbalance proposed by Robinson et al. (2018), sampling the data to artifically increase the prevalence of detections prior to modeling.\nSpatial precision: the spatial location of an eBird checklist is given as a single latitude-longitude point; however, this may not be precise for two main reasons. First, for traveling checklists, this location represents just one point on the journey. Second, eBird checklists are often assigned to a hotspot (a common location for all birders visiting a popular birding site) rather than their true location. For these reasons, it’s not appropriate to align the eBird locations with very precise habitat variables, and we recommend summarizing variables within a neighborhood around the checklist location.\nVariation in detectability/effort: detectability describes the probability of a species that is present in an area being detected and identified. Detectability varies by season, habitat, and species (Johnston et al. 2014, 2018). Furthermore, eBird data are collected with high variation in effort, time of day, number of observers, and external conditions such as weather, all of which can affect the detectability of species (Ellis and Taylor 2018; Oliveira et al. 2018). Therefore, detectability is particularly important to consider when comparing between seasons, habitats or species. Since eBird uses a semi-structured protocol, that collects data on the observation process, we’ll be able to account for a larger proportion of this variation in our analyses.\n\nThe remainder of this guide will demonstrate how to address these challenges using real data from eBird to produce reliable estimates of species distributions. In general, we’ll take a two-pronged approach to dealing with unstructured data and maximizing the value of citizen science data: imposing more structure onto the data via data filtering and including predictor variables describing the obsservation process in our models to account for the remaining variation."
  },
  {
    "objectID": "ebird.html#sec-ebird-download",
    "href": "ebird.html#sec-ebird-download",
    "title": "2  eBird Data",
    "section": "2.3 Downloading data",
    "text": "2.3 Downloading data\neBird data are typically distributed in two parts: observation data and checklist data. In the observation dataset, each row corresponds to the sighting of a single species on a checklist, including the count and any other species-level information (e.g. age, sex, species comments, etc.). In the checklist dataset, each row corresponds to a checklist, including the date, time, location, effort (e.g. distance traveled, time spent, etc.), and any additional checklist-level information (e.g. whether this is a complete checklist or not). The two datasets can be joined together using a unique checklist identifier (sometimes referred to as the sampling event identifier).\nThe observation and checklist data are released as tab-separated text files referred to as the eBird Basic Dataset (EBD) and the Sampling Event Data (SED), respectively. These files are released monthly and contain all validated bird sightings in the eBird database at the time of release. Both of these datasets can be downloaded in their entirety or a subset for a given species, region, or time period can be requested via the Custom Download form. We strongly recommend against attempting to download the complete EBD since it’s well over 100GB at the time of writing. Instead, we will demonstrate a workflow using the Custom Download approach. In what follows, we will assume you have followed the instructions for requesting access to eBird data outlined in the previous chapter.\n\n\n\nWood Thrush © Veronica Araya Garcia, Macaulay Library (ML60255811)\n\n\nIn the interest of making examples concrete, throughout this guide, we’ll focus on moedeling the distribution of Wood Thrush in Georgia (the US state, not the country) in June. Wood Thrush breed in deciduous forests of the eastern United States. We’ll start by downloading the corresponding eBird observation (EBD) and checklist (SED) data by visiting the eBird Basic Dataset download page and filling out the Custom Download form to request Wood Thrush observations from Georgia. Make sure you check the box “Include sampling event data”, which will include the SED in the data download in addition to the EBD.\n\nOnce the data are ready, you will receive an email with a download link. The downloaded data will be in a compressed .zip format, and should be unarchived. The resulting directory will contain a two text files: one for the EBD (e.g. ebd_US-GA_woothr_smp_relOct-2023.txt) containing all the Wood Thrush observations from Georgia and one for the SED (e.g. ebd_US-GA_woothr_smp_relOct-2023_sampling.txt) containing all checklists from Georgia. The relOct-2023 component of the file name describes which version of the EBD this dataset came from; in this case it’s the October 2023 release.\n\n\n\n\n\n\nTip\n\n\n\nSince the EBD is updated monthly, you will likely recieve a different version of the data than the October 2023 version used throughout the rest of this lesson. Provided you update the filenames of the downloaded files accordingly, the difference in versions will not be an issue. However, if you want to download and use exactly the same files used in this lesson, you can download the corresponding EBD zip file."
  },
  {
    "objectID": "ebird.html#sec-ebird-import",
    "href": "ebird.html#sec-ebird-import",
    "title": "2  eBird Data",
    "section": "2.4 Importing eBird data into R",
    "text": "2.4 Importing eBird data into R\nThe previous step left us with two tab separated text files, one for the EBD (i.e. observation data) and one for the SED (i.e. checklist data). For this example, we’ve placed the downloaded text files in the data-raw/ sub-directory of our working directory. Feel free to put these files in a place that’s convenient to you, but make sure to update the file paths in the following code blocks.\nThe auk functions read_ebd() or read_sampling() are designed to import the EBD and SED, respectively, into R. First let’s import the checklist data (SED).\n\nlibrary(auk)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(lubridate)\nlibrary(readr)\nlibrary(sf)\n\nf_sed &lt;- \"data-raw/ebd_US-GA_woothr_smp_relOct-2023_sampling.txt\"\nchecklists &lt;- read_sampling(f_sed)\nglimpse(checklists)\n#&gt; Rows: 1,120,132\n#&gt; Columns: 31\n#&gt; $ checklist_id              &lt;chr&gt; \"S78945522\", \"S100039674\", \"S7892023\", \"S789…\n#&gt; $ last_edited_date          &lt;chr&gt; \"2021-04-13 15:28:08.442812\", \"2022-01-03 14…\n#&gt; $ country                   &lt;chr&gt; \"United States\", \"United States\", \"United St…\n#&gt; $ country_code              &lt;chr&gt; \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"U…\n#&gt; $ state                     &lt;chr&gt; \"Georgia\", \"Georgia\", \"Georgia\", \"Georgia\", …\n#&gt; $ state_code                &lt;chr&gt; \"US-GA\", \"US-GA\", \"US-GA\", \"US-GA\", \"US-GA\",…\n#&gt; $ county                    &lt;chr&gt; \"Monroe\", \"Monroe\", \"Chatham\", \"Chatham\", \"C…\n#&gt; $ county_code               &lt;chr&gt; \"US-GA-207\", \"US-GA-207\", \"US-GA-051\", \"US-G…\n#&gt; $ iba_code                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ bcr_code                  &lt;int&gt; 29, 29, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ usfws_code                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ atlas_block               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ locality                  &lt;chr&gt; \"Rum Creek M.A.R.S.H. Project\", \"Rum Creek M…\n#&gt; $ locality_id               &lt;chr&gt; \"L876993\", \"L876993\", \"L877616\", \"L877614\", …\n#&gt; $ locality_type             &lt;chr&gt; \"H\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H\",…\n#&gt; $ latitude                  &lt;dbl&gt; 33.1, 33.1, 31.4, 31.7, 31.7, 31.7, 31.7, 31…\n#&gt; $ longitude                 &lt;dbl&gt; -83.9, -83.9, -80.2, -80.4, -80.4, -80.4, -8…\n#&gt; $ observation_date          &lt;date&gt; 1997-12-31, 1997-02-23, 1983-09-13, 1984-02…\n#&gt; $ time_observations_started &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, \"07:20:00\", \"15:…\n#&gt; $ observer_id               &lt;chr&gt; \"obs1675368\", \"obs198993\", \"obs243631\", \"obs…\n#&gt; $ sampling_event_identifier &lt;chr&gt; \"S78945522\", \"S100039674\", \"S7892023\", \"S789…\n#&gt; $ protocol_type             &lt;chr&gt; \"Historical\", \"Historical\", \"Incidental\", \"I…\n#&gt; $ protocol_code             &lt;chr&gt; \"P62\", \"P62\", \"P20\", \"P20\", \"P20\", \"P20\", \"P…\n#&gt; $ project_code              &lt;chr&gt; \"EBIRD\", \"EBIRD\", \"EBIRD\", \"EBIRD\", \"EBIRD\",…\n#&gt; $ duration_minutes          &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, 400, 120, NA, NA…\n#&gt; $ effort_distance_km        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 64.4, 40.2, NA, …\n#&gt; $ effort_area_ha            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ number_observers          &lt;int&gt; 1, NA, NA, NA, NA, NA, NA, 5, 15, NA, NA, NA…\n#&gt; $ all_species_reported      &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALS…\n#&gt; $ group_identifier          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ trip_comments             &lt;chr&gt; NA, NA, \"Joel McNeal entering accepted recor…\n\n\n\n\n\n\n\nCheckpoint\n\n\n\nTake some time to explore the variables in the checklist dataset. If you’re unsure about any of the variables, consult the metadata document that came with the data download (eBird_Basic_Dataset_Metadata_v1.15.pdf).\n\n\nFor some applications, only the checklist data are required. For example, the checklist data can be used to investigate the spatial and temporal distribution of eBird data within a region. This dataset can also be used to explore how much variation there is in the effort variables and identify checklists that have low spatial or temporal precision.\n\n\n\n\n\n\nExercise\n\n\n\nMake a histogram of the distribution of distance traveling for traveling protocol checklists.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMore than 95% of checklists are less than 10 km in length; however, some checklists are as long as 80 km in length. Long traveling checklists have lower spatial precision so they should generally be removed prior to analysis.\n\nchecklists_traveling &lt;- filter(checklists, protocol_type == \"Traveling\")\nggplot(checklists_traveling) +\n  aes(x = effort_distance_km) +\n  geom_histogram(binwidth = 1, \n                 aes(y = after_stat(count / sum(count)))) +\n  scale_y_continuous(limits = c(0, NA), labels = scales::label_percent()) +\n  labs(x = \"Distance traveled [km]\",\n       y = \"% of eBird checklists\",\n       title = \"Distribution of distance traveled on eBird checklists\")\n\n\n\n\n\n\n\n\n\n\n\nNow let’s import the observation data.\n\nf_ebd &lt;- \"data-raw/ebd_US-GA_woothr_smp_relOct-2023.txt\"\nobservations &lt;- read_ebd(f_ebd)\nglimpse(observations)\n#&gt; Rows: 41,432\n#&gt; Columns: 48\n#&gt; $ checklist_id              &lt;chr&gt; \"G10000432\", \"G10001259\", \"G10002209\", \"G100…\n#&gt; $ global_unique_identifier  &lt;chr&gt; \"URN:CornellLabOfOrnithology:EBIRD:OBS168581…\n#&gt; $ last_edited_date          &lt;chr&gt; \"2023-04-16 08:18:20.744008\", \"2023-05-08 08…\n#&gt; $ taxonomic_order           &lt;dbl&gt; 27880, 27880, 27880, 27880, 27880, 27880, 27…\n#&gt; $ category                  &lt;chr&gt; \"species\", \"species\", \"species\", \"species\", …\n#&gt; $ taxon_concept_id          &lt;chr&gt; \"avibase-8E1D9327\", \"avibase-8E1D9327\", \"avi…\n#&gt; $ common_name               &lt;chr&gt; \"Wood Thrush\", \"Wood Thrush\", \"Wood Thrush\",…\n#&gt; $ scientific_name           &lt;chr&gt; \"Hylocichla mustelina\", \"Hylocichla mustelin…\n#&gt; $ exotic_code               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ observation_count         &lt;chr&gt; \"1\", \"2\", \"2\", \"2\", \"4\", \"1\", \"1\", \"2\", \"1\",…\n#&gt; $ breeding_code             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ breeding_category         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ behavior_code             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ age_sex                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ country                   &lt;chr&gt; \"United States\", \"United States\", \"United St…\n#&gt; $ country_code              &lt;chr&gt; \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"U…\n#&gt; $ state                     &lt;chr&gt; \"Georgia\", \"Georgia\", \"Georgia\", \"Georgia\", …\n#&gt; $ state_code                &lt;chr&gt; \"US-GA\", \"US-GA\", \"US-GA\", \"US-GA\", \"US-GA\",…\n#&gt; $ county                    &lt;chr&gt; \"Bibb\", \"Laurens\", \"Wilkes\", \"Bartow\", \"Jasp…\n#&gt; $ county_code               &lt;chr&gt; \"US-GA-021\", \"US-GA-175\", \"US-GA-317\", \"US-G…\n#&gt; $ iba_code                  &lt;chr&gt; NA, NA, NA, NA, \"US-GA_3108\", NA, NA, NA, NA…\n#&gt; $ bcr_code                  &lt;int&gt; 27, 27, 29, 28, 29, 29, 29, 29, 29, 29, 29, …\n#&gt; $ usfws_code                &lt;chr&gt; NA, NA, NA, NA, \"USFWS_315\", NA, NA, NA, NA,…\n#&gt; $ atlas_block               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ locality                  &lt;chr&gt; \"Bondsview Rd., Macon\", \"Wendell Dixon Rd. D…\n#&gt; $ locality_id               &lt;chr&gt; \"L1286522\", \"L19345770\", \"L23606711\", \"L1118…\n#&gt; $ locality_type             &lt;chr&gt; \"H\", \"P\", \"P\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H\",…\n#&gt; $ latitude                  &lt;dbl&gt; 32.8, 32.7, 33.8, 34.2, 33.1, 33.9, 33.8, 33…\n#&gt; $ longitude                 &lt;dbl&gt; -83.6, -83.0, -82.8, -84.7, -83.8, -83.2, -8…\n#&gt; $ observation_date          &lt;date&gt; 2023-04-15, 2023-04-15, 2023-04-15, 2023-04…\n#&gt; $ time_observations_started &lt;chr&gt; \"08:18:00\", \"10:00:00\", \"09:11:00\", \"08:00:0…\n#&gt; $ observer_id               &lt;chr&gt; \"obsr139202,obsr617813,obsr198476,obsr267268…\n#&gt; $ sampling_event_identifier &lt;chr&gt; \"S133806681,S133806682,S133857533,S133858448…\n#&gt; $ protocol_type             &lt;chr&gt; \"Traveling\", \"Traveling\", \"Traveling\", \"Trav…\n#&gt; $ protocol_code             &lt;chr&gt; \"P22\", \"P22\", \"P22\", \"P22\", \"P22\", \"P22\", \"P…\n#&gt; $ project_code              &lt;chr&gt; \"EBIRD\", \"EBIRD\", \"EBIRD\", \"EBIRD\", \"EBIRD\",…\n#&gt; $ duration_minutes          &lt;int&gt; 57, 45, 102, 287, 170, 99, 85, 71, 32, 10, 4…\n#&gt; $ effort_distance_km        &lt;dbl&gt; 2.849, 9.656, 8.005, 5.404, 11.955, 5.501, 2…\n#&gt; $ effort_area_ha            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ number_observers          &lt;int&gt; 11, 2, 3, 12, 2, 3, 2, 2, 1, 1, 2, 2, 2, 2, …\n#&gt; $ all_species_reported      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n#&gt; $ group_identifier          &lt;chr&gt; \"G10000432\", \"G10001259\", \"G10002209\", \"G100…\n#&gt; $ has_media                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n#&gt; $ approved                  &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n#&gt; $ reviewed                  &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n#&gt; $ reason                    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ trip_comments             &lt;chr&gt; NA, \"Section of a 9.35 mile run I did today.…\n#&gt; $ species_comments          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\n\n\n\n\n\nCheckpoint\n\n\n\nTake some time to explore the variables in the observation dataset. Notice that the EBD duplicates many of the checklist-level variables from the SED.\n\n\nWhen any of the read functions from auk are used, three important processing steps occur by default behind the scenes.\n\nVariable name and type cleanup: The read functions assign clean variable names (in snake_case) and correct data types to all variables in the eBird datasets.\nCollapsing shared checklist: eBird allows sharing of checklists between observers part of the same birding event. These checklists lead to duplication or near duplication of records within the dataset and the function auk_unique(), applied by default by the auk read functions, addresses this by only keeping one independent copy of each checklist.\nTaxonomic rollup: eBird observations can be made at levels below species (e.g. subspecies) or above species (e.g. a bird that was identified as a duck, but the species could not be determined); however, for most uses we’ll want observations at the species level. auk_rollup() is applied by default when read_ebd() is used. It drops all observations not identifiable to a species and rolls up all observations reported below species to the species level.\n\nBefore proceeding, we’ll briefly demonstrate how shared checklists are collapsed and taxonomic rollup is performed. In practice, the auk read functions apply these processing steps by default and most data users will not have to worry about them.\n\n2.4.1 Shared checklists\neBird allows users to share checklists with other eBirders in their group, for example this checklist is shared by 8 observers. These checklists can be identified by looking at the group_identifier variable, which assigns an ID connecting all checklists in the group. To demonstrate this, we’ll read the checklist data in again, but with the argument unique = FALSE used to prevent read_sampling() from collapsing the shared checklists.\n\nchecklists_shared &lt;- read_sampling(f_sed, unique = FALSE)\n# identify shared checklists\nchecklists_shared |&gt; \n  filter(!is.na(group_identifier)) |&gt; \n  arrange(group_identifier) |&gt; \n  select(sampling_event_identifier, group_identifier)\n#&gt; # A tibble: 223,846 × 2\n#&gt;   sampling_event_identifier group_identifier\n#&gt;   &lt;chr&gt;                     &lt;chr&gt;           \n#&gt; 1 S19814680                 G1000012        \n#&gt; 2 S133802656                G10000310       \n#&gt; 3 S132144928                G10000310       \n#&gt; 4 S132144792                G10000330       \n#&gt; 5 S133803635                G10000330       \n#&gt; 6 S133859929                G10000432       \n#&gt; # ℹ 223,840 more rows\n\n\n\n\n\n\n\nTip\n\n\n\nSometimes it’s useful to inspect an eBird checklist online. You can view a checklist on the eBird website by appending the sampling_event_identifier to the URL https://ebird.org/checklist/. For example, to look at the checklist with ID S133864820, visit https://ebird.org/checklist/S19814680.\n\n\nChecklists with the same group_identifier provide duplicate information on the same birding event in the eBird database. For most analyses, it’s important to collapse these shared checklists down into a single checklist to avoid pseudoreplication. This can be accomplished with the function auk_unique(), which retains only one independent copy of each checklist.\n\nchecklists_unique &lt;- auk_unique(checklists_shared, checklists_only = TRUE)\nnrow(checklists_shared)\n#&gt; [1] 1249905\nnrow(checklists_unique)\n#&gt; [1] 1120132\n\nNotice that a new variable, checklist_id, was created that is set to group_identifier for shared checklists and sampling_event_identifier for non-shared checklists.\n\nhead(checklists_unique$checklist_id)\n#&gt; [1] \"S78945522\"  \"S100039674\" \"S7892023\"   \"S7892189\"   \"S7891643\"  \n#&gt; [6] \"S7891963\"\ntail(checklists_unique$checklist_id)\n#&gt; [1] \"G7637089\" \"G7637100\" \"G7637324\" \"G7637248\" \"G7637403\" \"G7640468\"\n\n\n\n\n\n\n\nTip\n\n\n\nCurious what checklists and observers contributed to a shared checklist after it has been collapsed? The sampling_event_identifier and observer_id contain comma-separated lists of all checklists and observers that went into the shared checklists.\n\nchecklists_unique |&gt; \n  filter(checklist_id == \"G7637089\") |&gt; \n  select(checklist_id, group_identifier, sampling_event_identifier, observer_id)\n#&gt; # A tibble: 1 × 4\n#&gt;   checklist_id group_identifier sampling_event_identifier observer_id        \n#&gt;   &lt;chr&gt;        &lt;chr&gt;            &lt;chr&gt;                     &lt;chr&gt;              \n#&gt; 1 G7637089     G7637089         S99977456,S99977457       obs325596,obs585422\n\n\n\n\n\n2.4.2 Taxonomic rollup\neBird observations can be made at levels below species (e.g. subspecies) or above species (e.g. a bird that was identified as a duck, but the species could not be determined); however, for most uses we’ll want observations at the species level. This is especially true if we want to produce detection/non-detection data from complete checklists because “complete” only applies at the species level.\nIn the example dataset used for this workshop, these taxonomic issues don’t apply. We have specifically requested Wood Thrush observations, so we haven’t received any observations for taxa above species, and Wood Thrush only no subspecies reportable in eBird. However, in many other situations, these taxonomic issues can be important. For example, this checklist has 10 Yellow-rumped Warblers, 5 each of two Yellow-rumped Warbler subspecies, and one hybrid between the two subspecies.\nThe function auk_rollup() drops all observations not identifiable to a species and “rolls up” all observations reported below species to the species level.\n\n\n\n\n\n\nTip\n\n\n\nIf multiple taxa on a single checklist roll up to the same species, auk_rollup() attempts to combine them intelligently. If each observation has a count, those counts are added together, but if any of the observations is missing a count (i.e. the count is “X”) the combined observation is also assigned an “X”. In the example checklist from the previous tip, with four taxa all rolling up to Yellow-rumped Warbler, auk_rollup() will add the four counts together to get 21 Yellow-rumped Warblers (10 + 5 + 5 + 1).\n\n\nTo demonstrate how taxonomic rollup works, we’ll use a small example dataset provided with the auk package.\n\n# import one of the auk example datasets without rolling up taxonomy\nobs_ex &lt;- system.file(\"extdata/ebd-rollup-ex.txt\", package = \"auk\") |&gt; \n  read_ebd(rollup = FALSE)\n# rollup taxonomy\nobs_ex_rollup &lt;- auk_rollup(obs_ex)\n\n# identify the taxonomic categories present in each dataset\nunique(obs_ex$category)\n#&gt; [1] \"domestic\"   \"form\"       \"hybrid\"     \"intergrade\" \"slash\"     \n#&gt; [6] \"spuh\"       \"species\"    \"issf\"\nunique(obs_ex_rollup$category)\n#&gt; [1] \"species\"\n\n# without rollup, there are four observations\nobs_ex |&gt;\n  filter(common_name == \"Yellow-rumped Warbler\") |&gt; \n  select(checklist_id, category, common_name, subspecies_common_name, \n         observation_count)\n#&gt; # A tibble: 4 × 5\n#&gt;   checklist_id category   common_name   subspecies_common_name observation_count\n#&gt;   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;                  &lt;chr&gt;            \n#&gt; 1 S44943108    intergrade Yellow-rumpe… Yellow-rumped Warbler… 1                \n#&gt; 2 S129851825   species    Yellow-rumpe… &lt;NA&gt;                   1                \n#&gt; 3 S129851825   issf       Yellow-rumpe… Yellow-rumped Warbler… 1                \n#&gt; 4 S129851825   issf       Yellow-rumpe… Yellow-rumped Warbler… 2\n# with rollup, they have been combined\nobs_ex_rollup |&gt;\n  filter(common_name == \"Yellow-rumped Warbler\") |&gt; \n  select(checklist_id, category, common_name, observation_count)\n#&gt; # A tibble: 2 × 4\n#&gt;   checklist_id category common_name           observation_count\n#&gt;   &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;                 &lt;chr&gt;            \n#&gt; 1 S129851825   species  Yellow-rumped Warbler 4                \n#&gt; 2 S44943108    species  Yellow-rumped Warbler 1"
  },
  {
    "objectID": "ebird.html#sec-ebird-filter",
    "href": "ebird.html#sec-ebird-filter",
    "title": "2  eBird Data",
    "section": "2.5 Filtering to study region and season",
    "text": "2.5 Filtering to study region and season\nThe Custom Download form allowed us to apply some basic filters to the eBird data we downloaded: we requested only Wood Thrush observations and only those on checklists from Georgia. However, in most cases you’ll want to apply additional spatial and/or temporal filters to the data that are specific to your study. For the examples used throughout this guide we’ll only want observations from June for the last 10 years (2014-2023). In addition, we’ll only use complete checklists (i.e., those for which all birds seen or heard were reported), which will allow us to produce detection/non-detection data. We can apply these filters using the filter() function from dplyr.\n\n# filter the checklist data\nchecklists &lt;- checklists |&gt; \n  filter(all_species_reported,\n         between(year(observation_date), 2014, 2023),\n         month(observation_date) == 6)\n\n# filter the observation data\nobservations &lt;- observations |&gt; \n  filter(all_species_reported,\n         between(year(observation_date), 2014, 2023),\n         month(observation_date) == 6)\n\nThe data we requested for Georgia using the Custom Download form will include checklists falling in the ocean off the coast of Georgia. Although these oceanic checklists are typically rare, it’s usually best to remove them when modeling a terrestrial species like Wood Thrush. We’ll using a boundary polygon for Georgia in the data/gis-data.gpkg file, buffered by 1 km, to filter our checklist data. A similar approach can be used if you’re interested in a custom region, for example, a national park for which you may have a shapefile defining the boundary.\n\n# convert checklist locations to points geometries\nchecklists_sf &lt;- checklists |&gt; \n  select(checklist_id, latitude, longitude) |&gt; \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# boundary of study region, buffered by 1 km\nstudy_region_buffered &lt;- read_sf(\"data/gis-data.gpkg\", layer = \"ne_states\") |&gt;\n  filter(state_code == \"US-GA\") |&gt;\n  st_transform(crs = st_crs(checklists_sf)) |&gt;\n  st_buffer(dist = 1000)\n\n# spatially subset the checklists to those in the study region\nin_region &lt;- checklists_sf[study_region_buffered, ]\n\n# join to checklists and observations to remove checklists outside region\nchecklists &lt;- semi_join(checklists, in_region, by = \"checklist_id\")\nobservations &lt;- semi_join(observations, in_region, by = \"checklist_id\")\n\n\n\n\n\n\n\nTip\n\n\n\nIt’s absolutely critical that we filter the observation and checklist data in exactly the same way to produce exactly the same population of checklists. Otherwise, the zero-filling we do in the next section will fail.\n\n\nFinally, there are rare situations in which some observers in a group of shared checklists quite drastically change there checklists, for example, changing the location or switching their checklist from complete to incomplete. In these cases, it’s possible to end up with a mismatch between the checklists in the observation dataset and the checklist dataset. We can resolve this very rare issue by removing any checklists from the observation dataset not appearing in the checklist dataset.\n\n# remove observations without matching checklists\nobservations &lt;- semi_join(observations, checklists, by = \"checklist_id\")"
  },
  {
    "objectID": "ebird.html#sec-ebird-zf",
    "href": "ebird.html#sec-ebird-zf",
    "title": "2  eBird Data",
    "section": "2.6 Zero-filling",
    "text": "2.6 Zero-filling\nTo a large degree, the power of eBird for rigorous analyses comes from the ability to transform the data to produce detection/non-detection data (also referred to as presence/absence data). With presence-only data, but no information of the amount of search effort expended to produce that data, it’s challenging even to estimate how rare or common a species is. For example, consider the 10 detections presented in the top row of the figure below, and ask yourself how common is this species? The bottom row of the figure presents three possible scenarios for the total number of checklists that generated the detections, from left to right\n\n50 checklists: in this case the species is fairly common with 20% of checklists reporting the species.\n250 checklists: in this case the species is uncommon with 4% of checklists reporting the species.\n1,000 checklists: in this case the species is rare with 1% of checklists reporting the species.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nThink of some real world scenarios where presence-only data could be a poor representation of the prevalence of a species.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are many cases of this phenomenon. A couple possible examples include:\n\nRare or particularly charismatic species will often have an inflated number of observations because observers specifically seek them out.\nHeavily populated (e.g. cities) or easily accessible areas (e.g. near roads) typically have more detections simply because more people visit them. In contrast, more remote areas may have better habitat for certain species, but have fewer observations because fewer birders are visiting these sites.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRemember that the prevalence of a species on eBird checklists (e.g. 10% of checklists detected a species) is only a relative measure of the actual occupancy probability of that species. To appear on an eBird checklist, a species must occur in an area and be detected by the observer. That detectability always plays a role in determining prevalence and it can vary drastically between regions, seasons, and species.\n\n\nThe EBD alone is a source of presence-only data, with one record for each taxon (typically species) reported. For complete checklists, information about non-detections can be inferred from the SED: if there is a record in the SED but no record for a species in the EBD, then a count of zero individuals of that species can be inferred. This process is referred to a “zero-filling” the data. We can use auk_zerofill() to combine the checklist and observation data together to produce zero-filled, detection/non-detection data.\n\nzf &lt;- auk_zerofill(observations, checklists, collapse = TRUE)\n\nBy default auk_zerofill() returns a compact representation of the data, consisting of a list of two data frames, one with checklist data and the other with observation data; the use of collapse = TRUE combines these into a single data frame, which will be easier to work with.\nBefore continuing, we’ll transform some of the variables to a more useful form for modelling. We convert time to a decimal value between 0 and 24, force the distance traveled to 0 for stationary checklists, and create a new variable for speed. Notably, eBirders have the option of entering an “X” rather than a count for a species, to indicate that the species was present, but they didn’t keep track of how many individuals were observed. During the modeling stage, we’ll want the observation_count variable stored as an integer and we’ll convert “X” to NA to allow for this.\n\n# function to convert time observation to hours since midnight\ntime_to_decimal &lt;- function(x) {\n  x &lt;- hms(x, quiet = TRUE)\n  hour(x) + minute(x) / 60 + second(x) / 3600\n}\n\n# clean up variables\nzf &lt;- zf |&gt; \n  mutate(\n    # convert count to integer and X to NA\n    # ignore the warning \"NAs introduced by coercion\"\n    observation_count = as.integer(observation_count),\n    # effort_distance_km to 0 for stationary counts\n    effort_distance_km = if_else(protocol_type == \"Stationary\", \n                                 0, effort_distance_km),\n    # convert duration to hours\n    effort_hours = duration_minutes / 60,\n    # speed km/h\n    effort_speed_kmph = effort_distance_km / effort_hours,\n    # convert time to decimal hours since midnight\n    hours_of_day = time_to_decimal(time_observations_started),\n    # split date into year and day of year\n    year = year(observation_date),\n    day_of_year = yday(observation_date)\n  )"
  },
  {
    "objectID": "ebird.html#sec-ebird-effort",
    "href": "ebird.html#sec-ebird-effort",
    "title": "2  eBird Data",
    "section": "2.7 Accounting for variation in effort",
    "text": "2.7 Accounting for variation in effort\nAs discussed in the Introduction, variation in effort between checklists makes inference challenging, because it is associated with variation in detectability. When working with semi-structured datasets like eBird, one approach to dealing with this variation is to impose some more consistent structure on the data by filtering observations on the effort variables. This reduces the variation in detectability between checklists. Based on our experience working with these data in the context of eBird Status and Trends, we suggest restricting checklists to traveling or stationary counts less than 6 hours in duration and 10 km in length, at speeds below 100km/h, and with 10 or fewer observers.\n\n# additional filtering\nzf_filtered &lt;- zf |&gt; \n  filter(protocol_type %in% c(\"Stationary\", \"Traveling\"),\n         effort_hours &lt;= 6,\n         effort_distance_km &lt;= 10,\n         effort_speed_kmph &lt;= 100,\n         number_observers &lt;= 10)\n\nNote that these filtering parameters are based on making predictions at weekly spatial resolution and 3 km spatial resolution. For applications requiring higher spatial precision, stricter filtering on effort_distance_km should be used.\n\n\n\n\n\n\nExercise\n\n\n\nPick one of the four effort variables we filtered on above and explore how much variation remains.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet’s pick the checklist duration and make a histogram. The large majority of checklists are well under the 6 hour cutoff, with more than half being less than an hour in duration.\n\nggplot(zf_filtered) +\n  aes(x = effort_hours) +\n  geom_histogram(binwidth = 0.5, \n                 aes(y = after_stat(count / sum(count)))) +\n  scale_y_continuous(limits = c(0, NA), labels = scales::label_percent()) +\n  labs(x = \"Duration [hours]\",\n       y = \"% of eBird checklists\",\n       title = \"Distribution of eBird checklist duration\")\n\n\n\n\n\n\n\n\n\n\n\n\n2.7.1 Spatial precision\nFor each eBird checklists we are provided a single location (longitude/latitude coordinates) specifying where the bird observations occurred. Ideally we want that checklist location to closely match where an observed bird was located. However, there are three main reasons why that may not be the case:\n\nThe bird and observer may not overlap in space. For example, raptors may be observed soaring at a great distance from the observer.\nThe checklist location may correspond to an eBird hotspot location rather than the location of the observer. eBird hotspots are public birding locations used to aggregate eBird data, for example, so all checklists at a park or wetland can be grouped together. If observers assign their checklists to an eBird hotspot, the coordinates in the ERD and SED will be those of the hotspot. In some cases, these locations will be a good representation of where the observations occurred, in others they may not be a good representation, for example, if the hotspot location is at the center of a large park but the observations occurred at the edge.\nTraveling checklists survey an area rather than a single point. The amount of area covered will depend on the distance traveled and the compactness of the route taken. For example, a 1 km checklist in a straight line may survey areas further from the checklist coordinates than a 10 km checklist that takes a very circuitous route.\n\nAll three of these factors impact the spatial precision of eBird data. Fortunately, many eBird checklists have GPS tracks associated with them. Although these tracks are not available for public use, we can use them to quantify the spatial precision of eBird data. In analyses to inform eBird Status and Trends, we calculated the centroid of all GPS tracks and estimated the distance between that centroid and the reported checklist location. For different maximum checklist distances, we can look at the cumulative distribution of checklists as a function of location error.\n\nAs a proxy for checklist compactness, we calculated the minimum radius of a circle fully containing the GPS track of each checklist. Again, we plot the cumulative distribution of this compactness measure for different maximum checklist distances.\n\nFor the specific example of checklists with distances less than 10 km (the cutoff applied above), 94% of traveling checklists are contained within a 1.5 km radius circle and 74% of traveling checklists have location error less than 500 m. This will inform the spatial scale that we use to make predictions in the next chapter. Depending on the precision required for your application, you can use the above plots to adjust the checklist effort filters to control spatial precision."
  },
  {
    "objectID": "ebird.html#sec-ebird-testtrain",
    "href": "ebird.html#sec-ebird-testtrain",
    "title": "2  eBird Data",
    "section": "2.8 Test-train split",
    "text": "2.8 Test-train split\nFor the modeling exercises used in this guide, we’ll hold aside a portion of the data from training to be used as an independent test set to assess the predictive performance of the model. Specifically, we’ll randomly split the data into 80% of checklists for training and 20% for testing. To facilitate this, we create a new variable type that will indicate whether the observation falls in the test set or training set.\n\nzf_filtered$type &lt;- if_else(runif(nrow(zf_filtered)) &lt;= 0.8, \"train\", \"test\")\n# confirm the proportion in each set is correct\ntable(zf_filtered$type) / nrow(zf_filtered)\n#&gt; \n#&gt;  test train \n#&gt;   0.2   0.8\n\nFinally, there are a large number of variables in the EBD that are redundant (e.g. both state names and codes are present) or unnecessary for most modeling exercises (e.g. checklist comments and Important Bird Area codes). These can be removed at this point, keeping only the variables we want for modelling. Then we’ll save the resulting zero-filled observations for use in later chapters.\n\nchecklists &lt;- zf_filtered |&gt; \n  select(checklist_id, observer_id, type,\n         observation_count, species_observed, \n         state_code, locality_id, latitude, longitude,\n         protocol_type, all_species_reported,\n         observation_date, year, day_of_year,\n         hours_of_day, \n         effort_hours, effort_distance_km, effort_speed_kmph,\n         number_observers)\nwrite_csv(checklists, \"data/checklists-zf_woothr_jun_us-ga.csv\", na = \"\")\n\nIf you’d like to ensure you’re using exactly the same data as was used to generate this guide, download the data package mentioned in the setup instructions. Unzip this data package and place the contents in your RStudio project folder."
  },
  {
    "objectID": "ebird.html#sec-ebird-explore",
    "href": "ebird.html#sec-ebird-explore",
    "title": "2  eBird Data",
    "section": "2.9 Exploratory analysis and visualization",
    "text": "2.9 Exploratory analysis and visualization\nBefore proceeding to training species distribution models with these data, it’s worth exploring the dataset to see what we’re working with. Let’s start by making a simple map of the observations. This map uses GIS data available for download in the data package. Unzip this data package and place the contents in your RStudio project folder.\n\n# load gis data\nne_land &lt;- read_sf(\"data/gis-data.gpkg\", \"ne_land\") |&gt; \n  st_geometry()\nne_country_lines &lt;- read_sf(\"data/gis-data.gpkg\", \"ne_country_lines\") |&gt; \n  st_geometry()\nne_state_lines &lt;- read_sf(\"data/gis-data.gpkg\", \"ne_state_lines\") |&gt; \n  st_geometry()\nstudy_region &lt;- read_sf(\"data/gis-data.gpkg\", \"ne_states\") |&gt; \n  filter(state_code == \"US-GA\") |&gt; \n  st_geometry()\n\n# prepare ebird data for mapping\nchecklists_sf &lt;- checklists |&gt; \n  # convert to spatial points\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) |&gt; \n  select(species_observed)\n\n# map\npar(mar = c(0.25, 0.25, 4, 0.25))\n# set up plot area\nplot(st_geometry(checklists_sf), \n     main = \"Wood Thrush eBird Observations\\nJune 2014-2023\",\n     col = NA, border = NA)\n# contextual gis data\nplot(ne_land, col = \"#cfcfcf\", border = \"#888888\", lwd = 0.5, add = TRUE)\nplot(study_region, col = \"#e6e6e6\", border = NA, add = TRUE)\nplot(ne_state_lines, col = \"#ffffff\", lwd = 0.75, add = TRUE)\nplot(ne_country_lines, col = \"#ffffff\", lwd = 1.5, add = TRUE)\n# ebird observations\n# not observed\nplot(filter(checklists_sf, !species_observed),\n     pch = 19, cex = 0.1, col = alpha(\"#555555\", 0.25),\n     add = TRUE)\n# observed\nplot(filter(checklists_sf, species_observed),\n     pch = 19, cex = 0.3, col = alpha(\"#4daf4a\", 1),\n     add = TRUE)\n# legend\nlegend(\"bottomright\", bty = \"n\",\n       col = c(\"#555555\", \"#4daf4a\"),\n       legend = c(\"eBird checklist\", \"Wood Thrush sighting\"),\n       pch = 19)\nbox()\n\n\n\n\n\n\n\n\nIn this map, the spatial bias in eBird data becomes immediately obvious, for example, notice the large number of checklists in areas around Atlanta, the largest city in Georgia, in the northern part of the state.\nExploring the effort variables is also a valuable exercise. For each effort variable, we’ll produce both a histogram and a plot of frequency of detection as a function of that effort variable. The histogram will tell us something about birder behavior. For example, what time of day are most people going birding, and for how long? We may also want to note values of the effort variable that have very few observations; predictions made in these regions may be unreliable due to a lack of data. The detection frequency plots tell us how the probability of detecting a species changes with effort.\n\n2.9.1 Time of day\nThe chance of an observer detecting a bird when present can be highly dependent on time of day. For example, many species exhibit a peak in detection early in the morning during dawn chorus and a secondary peak early in the evening. With this in mind, the first predictor of detection that we’ll explore is the time of day at which a checklist was started. We’ll summarize the data in 1 hour intervals, then plot them. Since estimates of detection frequency are unreliable when only a small number of checklists are available, we’ll only plot hours for which at least 100 checklists are present.\n\n# summarize data by hourly bins\nbreaks &lt;- seq(0, 24)\nlabels &lt;- breaks[-length(breaks)] + diff(breaks) / 2\nchecklists_time &lt;- checklists |&gt; \n  mutate(hour_bins = cut(hours_of_day, \n                         breaks = breaks, \n                         labels = labels,\n                         include.lowest = TRUE),\n         hour_bins = as.numeric(as.character(hour_bins))) |&gt; \n  group_by(hour_bins) |&gt; \n  summarise(n_checklists = n(),\n            n_detected = sum(species_observed),\n            det_freq = mean(species_observed))\n\n# histogram\ng_tod_hist &lt;- ggplot(checklists_time) +\n  aes(x = hour_bins, y = n_checklists) +\n  geom_segment(aes(xend = hour_bins, y = 0, yend = n_checklists),\n               color = \"grey50\") +\n  geom_point() +\n  scale_x_continuous(breaks = seq(0, 24, by = 3), limits = c(0, 24)) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(x = \"Hours since midnight\",\n       y = \"# checklists\",\n       title = \"Distribution of observation start times\")\n\n# frequency of detection\ng_tod_freq &lt;- ggplot(checklists_time |&gt; filter(n_checklists &gt; 100)) +\n  aes(x = hour_bins, y = det_freq) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = seq(0, 24, by = 3), limits = c(0, 24)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Hours since midnight\",\n       y = \"% checklists with detections\",\n       title = \"Detection frequency\")\n\n# combine\ngrid.arrange(g_tod_hist, g_tod_freq)\n\n\n\n\n\n\n\n\nAs expected, Wood Thrush detectability is highest early in the morning and quickly falls off as the day progresses. In later chapters, we’ll make predictions at the peak time of day for detectability to limit the effect of this variation. The majority of checklist submissions also occurs in the morning; however, there are reasonable numbers of checklists between 6am and 9pm. It’s in this region that our model estimates will be most reliable.\n\n\n2.9.2 Checklist duration\nWhen we filtered the eBird data in Section 2.7, we restricted observations to those from checklists 6 hours in duration or shorter to reduce variability. Let’s see what sort of variation remains in checklist duration.\n\n# summarize data by hour long bins\nbreaks &lt;- seq(0, 6)\nlabels &lt;- breaks[-length(breaks)] + diff(breaks) / 2\nchecklists_duration &lt;- checklists |&gt; \n  mutate(duration_bins = cut(effort_hours, \n                             breaks = breaks, \n                             labels = labels,\n                             include.lowest = TRUE),\n         duration_bins = as.numeric(as.character(duration_bins))) |&gt; \n  group_by(duration_bins) |&gt; \n  summarise(n_checklists = n(),\n            n_detected = sum(species_observed),\n            det_freq = mean(species_observed))\n\n# histogram\ng_duration_hist &lt;- ggplot(checklists_duration) +\n  aes(x = duration_bins, y = n_checklists) +\n  geom_segment(aes(xend = duration_bins, y = 0, yend = n_checklists),\n               color = \"grey50\") +\n  geom_point() +\n  scale_x_continuous(breaks = breaks) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(x = \"Checklist duration [hours]\",\n       y = \"# checklists\",\n       title = \"Distribution of checklist durations\")\n\n# frequency of detection\ng_duration_freq &lt;- ggplot(checklists_duration |&gt; filter(n_checklists &gt; 100)) +\n  aes(x = duration_bins, y = det_freq) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = breaks) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Checklist duration [hours]\",\n       y = \"% checklists with detections\",\n       title = \"Detection frequency\")\n\n# combine\ngrid.arrange(g_duration_hist, g_duration_freq)\n\n\n\n\n\n\n\n\nThe majority of checklists are an hour or shorter and there is a rapid decline in the frequency of checklists with increasing duration. In addition, longer searches yield a higher chance of detecting a Wood Thrush. In many cases, there is a saturation effect, with searches beyond a given length producing little additional benefit; however, here there appears to be a drop off in detection for checklists longer than 3.5 hours.\n\n\n2.9.3 Distance traveled\nAs with checklist duration, we expect a priori that the greater the distance someone travels, the greater the probability of encountering at least one Wood Thrush. Let’s see if this expectation is met. Note that we have already truncated the data to checklists less than 10 km in length.\n\n# summarize data by 1 km bins\nbreaks &lt;- seq(0, 10)\nlabels &lt;- breaks[-length(breaks)] + diff(breaks) / 2\nchecklists_dist &lt;- checklists |&gt; \n  mutate(dist_bins = cut(effort_distance_km, \n                         breaks = breaks, \n                         labels = labels,\n                         include.lowest = TRUE),\n         dist_bins = as.numeric(as.character(dist_bins))) |&gt; \n  group_by(dist_bins) |&gt; \n  summarise(n_checklists = n(),\n            n_detected = sum(species_observed),\n            det_freq = mean(species_observed))\n\n# histogram\ng_dist_hist &lt;- ggplot(checklists_dist) +\n  aes(x = dist_bins, y = n_checklists) +\n  geom_segment(aes(xend = dist_bins, y = 0, yend = n_checklists),\n               color = \"grey50\") +\n  geom_point() +\n  scale_x_continuous(breaks = breaks) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(x = \"Distance travelled [km]\",\n       y = \"# checklists\",\n       title = \"Distribution of distance travelled\")\n\n# frequency of detection\ng_dist_freq &lt;- ggplot(checklists_dist |&gt; filter(n_checklists &gt; 100)) +\n  aes(x = dist_bins, y = det_freq) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = breaks) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Distance travelled [km]\",\n       y = \"% checklists with detections\",\n       title = \"Detection frequency\")\n\n# combine\ngrid.arrange(g_dist_hist, g_dist_freq)\n\n\n\n\n\n\n\n\nAs with duration, the majority of observations are from short checklists (less than half a kilometer). One fortunate consequence of this is that most checklists will be contained within a small area within which habitat is not likely to show high variability. In Chapter 3, we will summarize land cover data within circles 3 km in diameter, centered on each checklist, and it appears that the vast majority of checklists will stay contained within this area.\n\n\n2.9.4 Number of observers\nFinally, let’s consider the number of observers whose observation are being reported in each checklist. We expect that at least up to some number of observers, reporting rates will increase; however, in working with these data we have found cases of declining detection rates for very large groups. With this in mind we have already restricted checklists to those with 30 or fewer observers, thus removing the very largest groups (prior to filtering, some checklists had as many as 180 observers!).\n\n# summarize data\nbreaks &lt;- seq(0, 10)\nlabels &lt;- seq(1, 10)\nchecklists_obs &lt;- checklists |&gt; \n  mutate(obs_bins = cut(number_observers, \n                        breaks = breaks, \n                        label = labels,\n                        include.lowest = TRUE),\n         obs_bins = as.numeric(as.character(obs_bins))) |&gt; \n  group_by(obs_bins) |&gt; \n  summarise(n_checklists = n(),\n            n_detected = sum(species_observed),\n            det_freq = mean(species_observed))\n\n# histogram\ng_obs_hist &lt;- ggplot(checklists_obs) +\n  aes(x = obs_bins, y = n_checklists) +\n  geom_segment(aes(xend = obs_bins, y = 0, yend = n_checklists),\n               color = \"grey50\") +\n  geom_point() +\n  scale_x_continuous(breaks = breaks) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(x = \"# observers\",\n       y = \"# checklists\",\n       title = \"Distribution of the number of observers\")\n\n# frequency of detection\ng_obs_freq &lt;- ggplot(checklists_obs |&gt; filter(n_checklists &gt; 100)) +\n  aes(x = obs_bins, y = det_freq) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = breaks) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"# observers\",\n       y = \"% checklists with detections\",\n       title = \"Detection frequency\")\n\n# combine\ngrid.arrange(g_obs_hist, g_obs_freq)\n\n\n\n\n\n\n\n\nThe majority of checklists have one or two observers and there appears to be an increase in detection frequency with more observers. However, it’s hard to distinguish a discernible pattern in the noise here, likely because there are so few checklists with more than 3 observers.\n\n\n\n\nCourter, Jason R., Ron J. Johnson, Claire M. Stuyck, Brian A. Lang, and Evan W. Kaiser. 2013. “Weekend Bias in Citizen Science Data Reporting: Implications for Phenology Studies.” International Journal of Biometeorology 57 (5): 715–20. https://doi.org/10.1007/s00484-012-0598-7.\n\n\nEllis, Murray V., and Jennifer E. Taylor. 2018. “Effects of Weather, Time of Day, and Survey Effort on Estimates of Species Richness in Temperate Woodlands.” Emu-Austral Ornithology 118 (2): 183–92.\n\n\nGreenwood, Jeremy J. D. 2007. “Citizens, Science and Bird Conservation.” Journal of Ornithology 148 (1): 77–124. https://doi.org/10.1007/s10336-007-0239-9.\n\n\nJohnston, Alison, Daniel Fink, Wesley M. Hochachka, and Steve Kelling. 2018. “Estimates of Observer Expertise Improve Species Distributions from Citizen Science Data.” Methods in Ecology and Evolution 9 (1): 88–97.\n\n\nJohnston, Alison, Stuart E. Newson, Kate Risely, Andy J. Musgrove, Dario Massimino, Stephen R. Baillie, and James W. Pearce-Higgins. 2014. “Species Traits Explain Variation in Detectability of UK Birds.” Bird Study 61 (3): 340–50.\n\n\nKadmon, Ronen, Oren Farber, and Avinoam Danin. 2004. “Effect of Roadside Bias on the Accuracy of Predictive Maps Produced by Bioclimatic Models.” Ecological Applications 14 (2): 401–13.\n\n\nKelling, Steve, Alison Johnston, Daniel Fink, Viviana Ruiz-Gutierrez, Rick Bonney, Aletta Bonn, Miguel Fernandez, et al. 2018. “Finding the Signal in the Noise of Citizen Science Observations.” bioRxiv, May, 326314. https://doi.org/10.1101/326314.\n\n\nLa Sorte, Frank A., Christopher A. Lepczyk, Jessica L. Burnett, Allen H. Hurlbert, Morgan W. Tingley, and Benjamin Zuckerberg. 2018. “Opportunities and Challenges for Big Data Ornithology.” The Condor 120 (2): 414–26.\n\n\nLuck, Gary W., Taylor H. Ricketts, Gretchen C. Daily, and Marc Imhoff. 2004. “Alleviating Spatial Conflict Between People and Biodiversity.” Proceedings of the National Academy of Sciences 101 (1): 182–86. https://doi.org/10.1073/pnas.2237148100.\n\n\nOliveira, Camilo Viana, Fabio Olmos, Manoel dos Santos-Filho, and Christine Steiner São Bernardo. 2018. “Observation of Diurnal Soaring Raptors In Northeastern Brazil Depends On Weather Conditions and Time of Day.” Journal of Raptor Research 52 (1): 56–65.\n\n\nPrendergast, J. R., S. N. Wood, J. H. Lawton, and B. C. Eversham. 1993. “Correcting for Variation in Recording Effort in Analyses of Diversity Hotspots.” Biodiversity Letters, 39–53.\n\n\nRobinson, Orin J., Viviana Ruiz-Gutierrez, Daniel Fink, Robert J. Meese, Marcel Holyoak, and Evan G. Cooch. 2018. “Using Citizen Science Data in Integrated Population Models to Inform Conservation Decision-Making.” bioRxiv, 293464.\n\n\nSullivan, Brian L., Jocelyn L. Aycrigg, Jessie H. Barry, Rick E. Bonney, Nicholas Bruns, Caren B. Cooper, Theo Damoulas, et al. 2014. “The eBird Enterprise: An Integrated Approach to Development and Application of Citizen Science.” Biological Conservation 169 (January): 31–40. https://doi.org/10.1016/j.biocon.2013.11.003.\n\n\nTulloch, Ayesha IT, and Judit K. Szabo. 2012. “A Behavioural Ecology Approach to Understand Volunteer Surveying for Citizen Science Datasets.” Emu-Austral Ornithology 112 (4): 313–25."
  },
  {
    "objectID": "envvar.html#sec-envvar-intro",
    "href": "envvar.html#sec-envvar-intro",
    "title": "3  Environmental Variables",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nSpecies distribution models work by finding associations between species occurrence or abundance and environmental variables. Using these relationships, it’s possible to predict the distribution in areas that aren’t sampled, provided we know the value of the environmental variables in these areas. Therefore, to proceed with the modeling in the next several chapters, we’ll need a suite of environmental variables to be used as predictors in our models. The particular set of environmental variables that’s most suitable for a given study will depend on the focal species, region, and time period, as well as the availability of data. When species distributions are well defined by the environmental variables, extrapolations to unsurveyed areas will be more accurate. So, it’s worth considering which variables are important for your species and region.\nFortunately, there are an abundance of freely available, satellite-based environmental datasets that are suitable for species distribution modeling. A small subset of possible data sources available globally includes data describing landcover, elevation, topography, surface water, and intertidal habitat. However, we encourage you to search for datasets suitable for their region and species of interest.\nSince there are such a wide range of available environmental datasets, and the distribution mechanisms and formats for each are different and often changing, we will not cover the specifics of how to download and pre-processes satellite-derived data products. Instead, we have downloaded and prepared example landcover and elevation datasets and will demonstrate how environmental variables can be extracted from these datasets in the following sections. This will provide examples of assigning environmental variables based on both categorical (landcover) and continuous (elevation) raster data sets.\nTo gain access to the example raster datasets, download the data package for this guide by following the instructions in the Introduction."
  },
  {
    "objectID": "envvar.html#sec-envvar-landcover",
    "href": "envvar.html#sec-envvar-landcover",
    "title": "3  Environmental Variables",
    "section": "3.2 Landcover",
    "text": "3.2 Landcover\nFor the examples in this guide, we’ll use land cover variables derived from the MODIS MCD12Q1 v006 land cover product (Friedl and Sulla-Menashe 2015). This product has global coverage at 500m spatial resolution and annual temporal resolution from 2001-2022. These data are available for several different classification schemes. We’ll use the University of Maryland classification system, which provides a globally accurate classification of land cover in our experience. This system classifies pixels into one of 15 different land cover classes. The terra package includes a nice tutorial for how to download and pre-processing MODIS data like this using the luna R package.\nA subset of the 2014-2022 data for our study region (i.e., Georgia) is in the data package. The file landcover_mcd12q1_umd_us-ga_2014-2022.tif should be in the data-raw/ subdirectory of your RStudio project. This is a multi-band GeoTIFF in which each band corresponds to a year of landcover data. In R, we’ll use the terra package to work with raster data.\n\nlibrary(dplyr)\nlibrary(exactextractr)\nlibrary(landscapemetrics)\nlibrary(readr)\nlibrary(sf)\nlibrary(stringr)\nlibrary(terra)\nlibrary(tidyr)\nlibrary(units)\nlibrary(viridis)\n\n# load and inspect the landcover data\nlandcover &lt;- rast(\"data-raw/landcover_mcd12q1_umd_us-ga_2014-2022.tif\")\nprint(landcover)\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 1174, 1249, 9  (nrow, ncol, nlyr)\n#&gt; resolution  : 463, 463  (x, y)\n#&gt; extent      : -8132528, -7553851, 3362724, 3906653  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#&gt; source      : landcover_mcd12q1_umd_us-ga_2014-2022.tif \n#&gt; names       : 2014, 2015, 2016, 2017, 2018, 2019, ... \n#&gt; min values  :    0,    0,    0,    0,    0,    0, ... \n#&gt; max values  :   15,   15,   15,   15,   15,   15, ...\n\n# map the data for 2022\nplot(as.factor(landcover[[\"2022\"]]), \n     main = \"MODIS Landcover 2022\",\n     axes = FALSE)\n\n\n\n\n\n\n\n\nWe’ve also included a lookup table in the data package (data-raw/mcd12q1_fao_classes.csv) that provides descriptions of each of these classes.\n\nlc_classes &lt;- read_csv(\"data-raw/mcd12q1_umd_classes.csv\")\nknitr::kable(lc_classes)\n\n\n\n\n\n\n\n\n\n\nclass\nname\nlabel\ndescription\n\n\n\n\n0\nWater bodies\nwater\nAt least 60% of area is covered by permanent water bodies.\n\n\n1\nEvergreen Needleleaf Forests\nevergreen_needleleaf\nDominated by evergreen conifer trees (canopy &gt;2m). Tree cover &gt;60%.\n\n\n2\nEvergreen Broadleaf Forests\nevergreen_broadleaf\nDominated by evergreen broadleaf and palmate trees (canopy &gt;2m). Tree cover &gt;60%.\n\n\n3\nDeciduous Needleleaf Forests\ndeciduous_needleleaf\nDominated by deciduous needleleaf (e.g. larch) trees (canopy &gt;2m). Tree cover &gt;60%.\n\n\n4\nDeciduous Broadleaf Forests\ndeciduous_broadleaf\nDominated by deciduous broadleaf trees (canopy &gt;2m). Tree cover &gt;60%.\n\n\n5\nMixed Forests\nmixed_forest\nDominated by neither deciduous nor evergreen (40-60% of each) tree type (canopy &gt;2m). Tree cover &gt;60%.\n\n\n6\nClosed Shrublands\nclosed_shrubland\nDominated by woody perennials (1-2m height) &gt;60% cover.\n\n\n7\nOpen Shrublands\nopen_shrubland\nDominated by woody perennials (1-2m height) 10-60% cover.\n\n\n8\nWoody Savannas\nwoody_savanna\nTree cover 30-60% (canopy &gt;2m).\n\n\n9\nSavannas\nsavanna\nTree cover 10-30% (canopy &gt;2m).\n\n\n10\nGrasslands\ngrassland\nDominated by herbaceous annuals (&lt;2m).\n\n\n12\nCroplands\ncropland\nAt least 60% of area is cultivated cropland.\n\n\n13\nUrban and Built-up Lands\nurban\nAt least 30% impervious surface area including building materials, asphalt, and vehicles.\n\n\n15\nNon-Vegetated Lands\nnonvegetated\nAt least 60% of area is non-vegetated barren (sand, rock, soil) or permanent snow and ice with less than 10% vegetation.\n\n\n255\nUnclassified\nunclassified\nHas not received a map label because of missing inputs.\n\n\n\n\n\nAt this point we could use the MODIS land cover data directly, simply extracting the land cover class for each checklist location. However, we instead advocate summarizing the land cover data within a neighborhood around the checklist locations. As discussed in Section 1.1, checklist locations are not precise, so it’s more appropriate to use the habitat in the surrounding area, rather than only at the checklist location. More fundamentally, organisms interact with their environment not at a single point, but at the scale of a landscape, so it’s important to include habitat information characterizing a suitably-sized landscape around the observation location. Based on our experience working with eBird data, a 3 km diameter circular neighborhood centered on each checklist location is sufficient to account for the spatial precision in the data when the maximum distance of travelling counts has been limited to 10km, while also being a relevant ecological scale for many bird species.\nThere are a variety of landscape metrics that can be used to characterize the composition (what habitat is available) and configuration (how that habitat is arranged spatially) of landscapes. Many of these metrics can be calculated using the R package landscapemetrics. We’ll use two simple metrics to summarize landcover data in this guide: percent landcover, a measure of composition, and edge density, a measure of configuration.\nFor example, we can consider a simplified landscape with three cover class: forest, grassland, and water. For each landcover class, percent landcover (abbreviated as pland) is the percent of the landscape that is composed of that class and edge density (abbreviated as ed) is the total boundary length of all patches of that class per unit area. For a broad range of scenarios, these two metrics are a reliable choice for calculating environmental covariates in distribution modeling.\n\n\n\n\n\n\n\n\n\nWe’ll start by finding the full set of unique checklists locations for each year in the eBird data and buffer the points by 1.5km to generate 3 km diameter circular neighborhoods centered on each checklist location. Note that the MODIS landcover data are not available for 2023, so we use the 2022 layer for 2023 checklists.\n\n# ebird checklist locations\nchecklists &lt;- read_csv(\"data/checklists-zf_woothr_jun_us-ga.csv\") |&gt; \n  # landcover data not availble for the full period, so we use the closest year\n  mutate(year_lc = as.character(pmin(year, 2022)))\n\n# generate circular neighborhoods for all checklists\nchecklists_sf &lt;- checklists |&gt; \n  # identify unique location/year combinations\n  distinct(locality_id, year_lc, latitude, longitude) |&gt; \n  # generate a 3 km neighborhoods\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\nbuffers &lt;- st_buffer(checklists_sf, dist = set_units(1.5, \"km\"))\n\nNext, for each location, we crop and mask the landcover layer corresponding to the checklist year to the circular neighborhood around that checklist. Then we use calculate_lsm() from landscapemetrics to calculate pland and ed metrics within each neighborhood. This step may take 30 minute or longer to run.\n\nlsm &lt;- NULL\nfor (i in seq_len(nrow(buffers))) {\n  buffer_i &lt;- st_transform(buffers[i, ], crs = crs(landcover))\n  year &lt;- as.character(buffer_i$year_lc)\n  \n  # crop and mask landcover raster so all values outside buffer are missing\n  lsm[[i]] &lt;- crop(landcover[[year]], buffer_i) |&gt; \n    mask(buffer_i) |&gt; \n    # calcualte landscape metrics\n    calculate_lsm(level = \"class\", metric = c(\"pland\", \"ed\")) |&gt; \n    # add variables to uniquely identify each point\n    mutate(locality_id = buffer_i$locality_id, \n           year_lc = buffer_i$year_lc) |&gt; \n    select(locality_id, year_lc, class, metric, value)\n}\nlsm &lt;- bind_rows(lsm)\n\nFinally, we’ll transform the data frame so there’s one row per location and all the environmental variables appear as columns. For each location, any landcover classes that don’t appear within the buffer will not have associated pland and ed metrics; at this stage, we replace these implicit missing values with zeros using the complete list of classes in lc_classes. We also replace the opaque class numbers with more meaningful names from the class description file data-raw/mcd12q1_umd_classes.csv.\n\nlsm_wide &lt;- lsm |&gt; \n  # fill missing classes with zeros\n  complete(nesting(locality_id, year_lc),\n           class = lc_classes$class,\n           metric = c(\"ed\", \"pland\"),\n           fill = list(value = 0)) |&gt; \n  # bring in more descriptive names\n  inner_join(select(lc_classes, class, label), by = \"class\") |&gt; \n  # transform from long to wide format\n  pivot_wider(values_from = value,\n              names_from = c(class, label, metric),\n              names_glue = \"{metric}_c{str_pad(class, 2, pad = '0')}_{label}\",\n              names_sort = TRUE) |&gt; \n  arrange(locality_id, year_lc)"
  },
  {
    "objectID": "envvar.html#sec-envvar-elevation",
    "href": "envvar.html#sec-envvar-elevation",
    "title": "3  Environmental Variables",
    "section": "3.3 Elevation",
    "text": "3.3 Elevation\nIn this section we’ll demonstrate how to assign elevation variables, which frequently play an important role in shaping species distributions. Amatulli et al. (2018) provide a suite of global, 1 km resolution topographic variables designed for use in distribution modeling. A range of variables are available, including elevation, slope, roughness, and many others; we’ll focus on elevation here, but the approach can easily be applied to other variables.\nAll the elevation and topography variables, at various resolutions, are available for download via the website. However, we’ve provided a subset of the 1 km resolution median elevation product covering our study extent data package for this guide. The file elevation_gmted_1km_us-ga.tif should be in the data-raw/ subdirectory of your RStudio project.\nAnalogous to how we assigned landcover variables, we’ll calculate the mean and standard deviation of the elevation within 3 km diameter circular neighborhoods centered on each checklist location.\n\n# elevation raster\nelevation &lt;- rast(\"data-raw/elevation_gmted_1km_us-ga.tif\")\n\n# mean and standard deviation within each circular neighborhood\nelev_buffer &lt;- exact_extract(elevation, buffers, fun = c(\"mean\", \"stdev\"),\n                             progress = FALSE) |&gt; \n  # add variables to uniquely identify each point\n  mutate(locality_id = buffers$locality_id, year_lc = buffers$year_lc) |&gt; \n  select(locality_id, year_lc, \n         elevation_mean = mean,\n         elevation_sd = stdev)\n\nNow, let’s combine the landcover and elevation variables together, join them back to the checklist data, and save them to be used as model predictors in the upcoming chapters.\n\n# combine elevation and landcover\nenv_variables &lt;- inner_join(elev_buffer, lsm_wide,\n                            by = c(\"locality_id\", \"year_lc\"))\n\n# attach and expand to checklists\nenv_variables &lt;- checklists |&gt; \n  select(checklist_id, locality_id, year_lc) |&gt; \n  inner_join(env_variables, by = c(\"locality_id\", \"year_lc\")) |&gt; \n  select(-locality_id, -year_lc)\n\n# save to csv, dropping any rows with missing variables\nwrite_csv(drop_na(env_variables), \n          \"data/environmental-variables_checklists_jun_us-ga.csv\", \n          na = \"\")"
  },
  {
    "objectID": "envvar.html#sec-envvar-pred",
    "href": "envvar.html#sec-envvar-pred",
    "title": "3  Environmental Variables",
    "section": "3.4 Prediction grid",
    "text": "3.4 Prediction grid\nAfter training a species distribution model, the goal is typically to make predictions throughout the study area. To do this, we’ll need a prediction grid: a regular grid of habitat variables over which to make predictions. In this section, we’ll create such a prediction grid for our study region (Georgia) using the MODIS landcover data from the most recent year for which they’re available in addition to elevation data. To start, we’ll create a template raster with cells equal in dimension to the diameter of the circular neighborhoods we used above. It’s important to use an equal area coordinate reference system for the prediction grid; we’ll use a Lambert’s azimuthal equal area projection centered on our study region.\n\n\n\n\n\n\nTip\n\n\n\nLambert’s azimuthal equal area projection is a good coordinate reference system to use for regional analysis and mapping. It can be tailored to a region of interest by setting the reference latitude and longitude in the projection string to the center of your study region. You can find the coordinates of a region using a mapping tool like Google Maps. Alternatively, you can calculate the centroid of your data using the sf packagage function st_centroid().\n\nchecklists_sf |&gt; \n  st_union() |&gt; \n  st_centroid() |&gt; \n  st_coordinates() |&gt; \n  round(1)\n#&gt;          X    Y\n#&gt; [1,] -83.7 33.2\n\n\n\n\n# lambert's azimuthal equal area projection for georgia\nlaea_crs &lt;- st_crs(\"+proj=laea +lat_0=33.2 +lon_0=-83.7\")\n\n# study region: georgia\nstudy_region &lt;- read_sf(\"data/gis-data.gpkg\", layer = \"ne_states\") |&gt; \n  filter(state_code == \"US-GA\") |&gt; \n  st_transform(crs = laea_crs)\n\n# create a raster template covering the region with 3 km resolution\nr &lt;- rast(study_region, res = c(3000, 3000))\n\n# fill the raster with 1s inside the study region\nr &lt;- rasterize(study_region, r, values = 1) |&gt; \n  setNames(\"study_region\")\n\n# save for later use\nr &lt;- writeRaster(r, \"data/prediction-grid_us-ga.tif\",\n                 overwrite = TRUE,\n                 gdal = \"COMPRESS=DEFLATE\")\n\nNext, we extract the coordinates of the cell centers from the raster we just created, convert these to sf point features, and buffer these to generate 3 km circular neighborhoods.\n\n# generate neighborhoods for the prediction grid cell centers\nbuffers_pg &lt;- as.data.frame(r, cells = TRUE, xy = TRUE) |&gt; \n  select(cell_id = cell, x, y) |&gt; \n  st_as_sf(coords = c(\"x\", \"y\"), crs = laea_crs, remove = FALSE) |&gt; \n  st_transform(crs = 4326) |&gt; \n  st_buffer(set_units(3, \"km\"))\n\nNow we can calculate the landcover and elevation variables exactly as we did for the eBird checklists in the previous two sections. First, the landscape metrics pland and ed from the landcover data. Note that we use the most recent year of landcover data (i.e. 2022) in this case.\n\n# estimate landscape metrics for each cell in the prediction grid\nlsm_pg &lt;- NULL\nfor (i in seq_len(nrow(buffers_pg))) {\n  buffer_i &lt;- st_transform(buffers_pg[i, ], crs = crs(landcover))\n  \n  # crop and mask landcover raster so all values outside buffer are missing\n  lsm_pg[[i]] &lt;- crop(landcover[[\"2022\"]], buffer_i) |&gt; \n    mask(buffer_i) |&gt; \n    # calcualte landscape metrics\n    calculate_lsm(level = \"class\", metric = c(\"pland\", \"ed\")) |&gt; \n    # add variable to uniquely identify each point\n    mutate(cell_id = buffer_i$cell_id) |&gt; \n    select(cell_id, class, metric, value)\n}\nlsm_pg &lt;- bind_rows(lsm_pg)\n\n# transform to wide format\nlsm_wide_pg &lt;- lsm_pg |&gt; \n  # fill missing classes with zeros\n  complete(cell_id,\n           class = lc_classes$class,\n           metric = c(\"ed\", \"pland\"),\n           fill = list(value = 0)) |&gt; \n  # bring in more descriptive names\n  inner_join(select(lc_classes, class, label), by = \"class\") |&gt; \n  # transform from long to wide format\n  pivot_wider(values_from = value,\n              names_from = c(class, label, metric),\n              names_glue = \"{metric}_c{str_pad(class, 2, pad = '0')}_{label}\",\n              names_sort = TRUE,\n              values_fill = 0) |&gt; \n  arrange(cell_id)\n\nAnd now the mean and standard deviation of elevation.\n\nelev_buffer_pg &lt;- exact_extract(elevation, buffers_pg, \n                                fun = c(\"mean\", \"stdev\"),\n                                progress = FALSE) |&gt; \n  # add variables to uniquely identify each point\n  mutate(cell_id = buffers_pg$cell_id) |&gt; \n  select(cell_id, elevation_mean = mean, elevation_sd = stdev)\n\nFinally, we combine the landcover and elevation variables together and save to CSV.\n\n# combine landcover and elevation\nenv_variables_pg &lt;- inner_join(elev_buffer_pg, lsm_wide_pg, by = \"cell_id\")\n\n# attach the xy coordinates of the cell centers\nenv_variables_pg &lt;- buffers_pg |&gt; \n  st_drop_geometry() |&gt; \n  select(cell_id, x, y) |&gt; \n  inner_join(env_variables_pg, by = \"cell_id\")\n\n# save as csv, dropping any rows with missing variables\nwrite_csv(drop_na(env_variables_pg),\n          \"data/environmental-variables_prediction-grid_us-ga.csv\", \n          na = \"\")\n\nKeeping these data in a data frame is a compact way to store them and will be required once we make model predictions in later chapters. However, we can always use the raster template to convert these environmental variables into a spatial format, for example, if we want to map them. Let’s look at how this works for percent cover of deciduous broadleaf forest (class 4).\n\nforest_cover &lt;- env_variables_pg |&gt; \n  # convert to spatial features\n  st_as_sf(coords = c(\"x\", \"y\"), crs = laea_crs) |&gt; \n  # rasterize points\n  rasterize(r, field = \"pland_c04_deciduous_broadleaf\")\n\n# make a map\npar(mar = c(0.25, 0.25, 2, 0.25))\nplot(forest_cover, \n     axes = FALSE, box = FALSE, col = viridis(10), \n     main = \"Deciduous Broadleaf Forest (% cover)\")\n\n\n\n\n\n\n\n\n\n\n\n\nAmatulli, Giuseppe, Sami Domisch, Mao-Ning Tuanmu, Benoit Parmentier, Ajay Ranipeta, Jeremy Malczyk, and Walter Jetz. 2018. “A Suite of Global, Cross-Scale Topographic Variables for Environmental and Biodiversity Modeling.” Scientific Data 5 (March): 180040. https://doi.org/10.1038/sdata.2018.40.\n\n\nFriedl, Mark, and Damien Sulla-Menashe. 2015. “MCD12Q1 MODIS/Terra+Aqua Land Cover Type Yearly L3 Global 500m SIN Grid V006.” NASA EOSDIS Land Processes DAAC. https://doi.org/10.5067/MODIS/MCD12Q1.006."
  },
  {
    "objectID": "encounter.html#sec-encounter-intro",
    "href": "encounter.html#sec-encounter-intro",
    "title": "4  Encounter Rate",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nIn this chapter we’ll estimate the encounter rate of Wood Thrush on eBird checklists in June in the state of Georgia. We define encounter rate as measuring the probability of an eBirder encountering a species on a standard eBird checklist.\nThe ecological metric we’re ultimately interested in is the probability that a species occurs at a site (i.e. the occupancy probability). This is usually not possible to estimate with semi-structured citizen science data like those from eBird because we typically can’t estimate absolute detectability. However, by accounting for much of the variation in detectability by including effort covariates in our model, the remaining unaccounted detectability will be more consistent across sites (Guillera-Arroita et al. 2015). Therefore, the encounter rate metric will be proportional to occupancy, albeit lower by some consistent amount. For some easily detectable species the difference between occurrence and actual occupancy rate will be small, and these encounter rates will approximate the actual occupancy rates of the species. For harder to detect species, the encounter rate may be substantially lower than the occupancy rate.\nRandom forests are a general purpose machine learning method applicable to a wide range of classification and regression problems, including the task at hand: classifying detection and non-detection of a species on eBird checklists. In addition to having good predictive performance, random forests are reasonably easy to use and have several efficient implementations in R. Prior to training a random forests model, we’ll demonstrate how to address issues of class imbalance and spatial bias using spatial subsampling on a regular grid. After training the model, we’ll assess its performance using a subset of data put aside for testing, and calibrate the model to ensure predictions are accurate. Finally, we’ll predict encounter rates throughout the study area and produce maps of these predictions."
  },
  {
    "objectID": "encounter.html#sec-encounter-data",
    "href": "encounter.html#sec-encounter-data",
    "title": "4  Encounter Rate",
    "section": "4.2 Data preparation",
    "text": "4.2 Data preparation\nLet’s get started by loading the necessary packages and data. If you worked through the previous chapters, you should have all the data required for this chapter. However, you may want to download the data package, and unzip it to your project directory, to ensure you’re working with exactly the same data as was used in the creation of this guide.\n\nlibrary(dplyr)\nlibrary(ebirdst)\nlibrary(fields)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(lubridate)\nlibrary(mccf1)\nlibrary(ranger)\nlibrary(readr)\nlibrary(scam)\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyr)\n\n# set random number seed for reproducibility\nset.seed(1)\n\n# environmental variables: landcover and elevation\nenv_vars &lt;- read_csv(\"data/environmental-variables_checklists_jun_us-ga.csv\")\n\n# zero-filled ebird data combined with environmental data\nchecklists_env &lt;- read_csv(\"data/checklists-zf_woothr_jun_us-ga.csv\") |&gt; \n  inner_join(env_vars, by = \"checklist_id\")\n\n# prediction grid\npred_grid &lt;- read_csv(\"data/environmental-variables_prediction-grid_us-ga.csv\")\n# raster template for the grid\nr &lt;- rast(\"data/prediction-grid_us-ga.tif\")\n# get the coordinate reference system of the prediction grid\ncrs &lt;- st_crs(r)\n\n# load gis data for making maps\nstudy_region &lt;- read_sf(\"data/gis-data.gpkg\", \"ne_states\") |&gt; \n  filter(state_code == \"US-GA\") |&gt; \n  st_transform(crs = crs) |&gt; \n  st_geometry()\nne_land &lt;- read_sf(\"data/gis-data.gpkg\", \"ne_land\") |&gt; \n  st_transform(crs = crs) |&gt; \n  st_geometry()\nne_country_lines &lt;- read_sf(\"data/gis-data.gpkg\", \"ne_country_lines\") |&gt; \n  st_transform(crs = crs) |&gt; \n  st_geometry()\nne_state_lines &lt;- read_sf(\"data/gis-data.gpkg\", \"ne_state_lines\") |&gt; \n  st_transform(crs = crs) |&gt; \n  st_geometry()"
  },
  {
    "objectID": "encounter.html#sec-encounter-sss",
    "href": "encounter.html#sec-encounter-sss",
    "title": "4  Encounter Rate",
    "section": "4.3 Spatiotemporal subsampling",
    "text": "4.3 Spatiotemporal subsampling\nAs discussed in the introduction, three of the challenges faced when using eBird data, are spatial bias, temporal bias, and class imbalance. Spatial and temporal bias refers to the tendency of eBird checklists to be distributed non-randomly in space and time, while class imbalance is the phenomenon that there are many more non-detections than detections for most species. All three can impact our ability to make reliable inferences from these data. Fortunately, all three can largely be addressed through subsampling the eBird data prior to modeling. In particular, we define an equal area, 3 km by 3 km square grid across the study region, then subsample detections and non-detections independently from the grid to ensure that we don’t lose too many detections. To address temporal bias, we’ll sample one detection and one non-detection checklist from each grid cell for each week of each year. Fortunately, the package ebirdst has a function grid_sample_stratified() that is specifically design to perform this type of sampling on eBird checklist data.\nBefore working with the real data, it’s instructive to look at a simple toy example, to see how this subsampling process works.\n\n# generate random points for a single week of the year\npts &lt;- data.frame(longitude = runif(500, -0.1, 0.1),\n                  latitude = runif(500, -0.1, 0.1),\n                  day_of_year = sample(1:7, 500, replace = TRUE))\n\n# sample one checklist per grid cell\n# by default grid_sample() uses a 3km x 3km x 1 week grid\npts_ss &lt;- grid_sample(pts)\n\n# generate polygons for the grid cells\nggplot(pts) +\n  aes(x = longitude, y = latitude) +\n  geom_point(size = 0.5) +\n  geom_point(data = pts_ss, col = \"red\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIn the above plot, the full set of points is shown in black and the subsampled points are shown in red. Now let’s apply exactly the same approach to subsampling the real eBird checklists; however, now we subsample temporally in addition to spatially, and sample detections and non-detections separately. Also, recall from Section 2.8 that we split the data 80/20 into train/test sets. Using the sample_by argument to grid_sample_stratified(), we can independently sample from the train and test sets to remove bias from both.\n\n# sample one checklist per 3km x 3km x 1 week grid for each year\n# sample detection/non-detection independently \nchecklists_ss &lt;- grid_sample_stratified(checklists_env,\n                                        obs_column = \"species_observed\",\n                                        sample_by = \"type\")\n\n\n\n\n\n\n\nExercise\n\n\n\nCompare the full set of eBird checklists to the set of checklists remaining after subsampling. What was the change in sampled size and how did the subsampling impact the prevalence of detections compared to non-detections?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe subsampling decreased the overall number of checklists by a factor of about 2.3, but increased the prevalence of detections from 10.7% to 14.0%.\n\n# original data\nnrow(checklists_env)\n#&gt; [1] 47863\ncount(checklists_env, species_observed) |&gt; \n  mutate(percent = n / sum(n))\n#&gt; # A tibble: 2 × 3\n#&gt;   species_observed     n percent\n#&gt;   &lt;lgl&gt;            &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1 FALSE            42724   0.893\n#&gt; 2 TRUE              5139   0.107\n\n# after sampling\nnrow(checklists_ss)\n#&gt; [1] 20642\ncount(checklists_ss, species_observed) |&gt; \n  mutate(percent = n / sum(n))\n#&gt; # A tibble: 2 × 3\n#&gt;   species_observed     n percent\n#&gt;   &lt;lgl&gt;            &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1 FALSE            17744   0.860\n#&gt; 2 TRUE              2898   0.140\n\n\n\n\nThis increase in the prevalence of detections will help the random forests model distinguish where birds are being observed; however, as s a result of this increase, the estimated encounter rate based on these subsampled data will be larger than the true encounter rate. When examining the outputs from the models it will be important to recall that we altered the prevalence rate at this stage. Now let’s look at how the subsampling affects the spatial distribution of the training observations.\n\n# convert checklists to spatial features\nall_pts &lt;- checklists_env |&gt;  \n  filter(type == \"train\") |&gt; \n  st_as_sf(coords = c(\"longitude\",\"latitude\"), crs = 4326) |&gt;\n  st_transform(crs = crs) |&gt; \n  select(species_observed)\nss_pts &lt;- checklists_ss |&gt;  \n  filter(type == \"train\") |&gt; \n  st_as_sf(coords = c(\"longitude\",\"latitude\"), crs = 4326) |&gt;\n  st_transform(crs = crs) |&gt; \n  select(species_observed)\nboth_pts &lt;- list(before_ss = all_pts, after_ss = ss_pts)\n\n# map\np &lt;- par(mfrow = c(1, 2))\nfor (i in seq_along(both_pts)) {\n  par(mar = c(0.25, 0.25, 0.25, 0.25))\n  # set up plot area\n  plot(st_geometry(both_pts[[i]]), col = NA)\n  # contextual gis data\n  plot(ne_land, col = \"#dddddd\", border = \"#888888\", lwd = 0.5, add = TRUE)\n  plot(study_region, col = \"#cccccc\", border = NA, add = TRUE)\n  plot(ne_state_lines, col = \"#ffffff\", lwd = 0.75, add = TRUE)\n  plot(ne_country_lines, col = \"#ffffff\", lwd = 1.5, add = TRUE)\n  # ebird observations\n  # not observed\n  plot(st_geometry(both_pts[[i]]),\n       pch = 19, cex = 0.1, col = alpha(\"#555555\", 0.25),\n       add = TRUE)\n  # observed\n  plot(filter(both_pts[[i]], species_observed) |&gt; st_geometry(),\n       pch = 19, cex = 0.3, col = alpha(\"#4daf4a\", 0.5),\n       add = TRUE)\n  # legend\n  legend(\"bottomright\", bty = \"n\",\n         col = c(\"#555555\", \"#4daf4a\"),\n         legend = c(\"Non-detection\", \"Detection\"),\n         pch = 19)\n  box()\n  par(new = TRUE, mar = c(0, 0, 3, 0))\n  if (names(both_pts)[i] == \"before_ss\") {\n    title(\"Wood Thrush eBird Observations\\nBefore subsampling\")\n  } else {\n    title(\"After subsampling\")\n  }\n}\npar(p)\n\n\n\n\n\n\n\n\nFor Wood Thrush, subsampling the detections and non-detections independently is sufficient for dealing with class imbalance. You can assess the impact of class imbalance by looking at the prevalence rates and examining whether the models are good at predicting to validation data. For species that are extremely rare, it may be worthwhile considering keeping all detections or even oversampling detections (Robinson, Ruiz-Gutierrez, and Fink 2018). In doing this, be aware that some of your species detections will not be independent, which could lead to overfitting of the data. Overall, when thinking about the number of detections and the prevalence rate, it’s important to consider both the ecology and detectability of the focal species, and the behavior of observers towards this species."
  },
  {
    "objectID": "encounter.html#sec-encounter-rf",
    "href": "encounter.html#sec-encounter-rf",
    "title": "4  Encounter Rate",
    "section": "4.4 Random forests",
    "text": "4.4 Random forests\nNow we’ll use a random forests model to relate detection/non-detection of Wood Thrush to the environmental variables we calculated in Chapter 3 (MODIS land cover and elevation), while also accounting for variation in detectability by including a suite of effort covariates. At this stage, we filter to just the training set, leaving the test set to assess predictive performance later.\n\nchecklists_train &lt;- checklists_ss |&gt; \n  filter(type == \"train\") |&gt; \n  # select only the columns to be used in the model\n  select(species_observed,\n         year, day_of_year, hours_of_day,\n         effort_hours, effort_distance_km, effort_speed_kmph,\n         number_observers, \n         starts_with(\"pland_\"),\n         starts_with(\"ed_\"),\n         starts_with(\"elevation_\"))\n\nAlthough we were able to partially address the issue of class imbalance via subsampling, detections still only make up 14.0% of observations, and for rare species this number will be even lower. Most classification algorithms aim to minimize the overall error rate, which results in poor predictive performance for rare classes (Chen, Liaw, and Breiman 2004). To address this issue, we’ll use a balanced random forest approach, a modification of the traditional random forest algorithm designed to handle imbalanced data. In this approach, each of the trees that makes up the random forest is generated using a random sample of the data chosen such that there is an equal number of the detections (the rare class) and non-detections (the common class). To use this approach, we’ll need to calculate the proportion of detections in the dataset.\n\ndetection_freq &lt;- mean(checklists_train$species_observed)\n\nThere are several packages for training random forests in R; however, we’ll use ranger, which is a very fast implementation with all the features we need. To fit a balanced random forests, we use the sample.fraction parameter to instruct ranger to grow each tree based on a random sample of the data that has an equal number of detections and non-detections. Specifying this is somewhat obtuse, because we need to tell ranger the proportion of the total data set to sample for non-detections and detections, and when this proportion is the same as the proportion of the rarer class–the detections–then then ranger will sample from all of the rarer class but from an equally sized subset of the more common non-detections. We use replace = TRUE to ensure that it’s a bootstrap sample. We’ll also ask ranger to predict probabilities, rather than simply returning the most probable class, with probability = TRUE.\n\n# ranger requires a factor response to do classification\ner_model &lt;- ranger(formula =  as.factor(species_observed) ~ ., \n                   data = checklists_train,\n                   importance = \"impurity\",\n                   probability = TRUE,\n                   replace = TRUE, \n                   sample.fraction = c(detection_freq, detection_freq))\n\n\n4.4.1 Calibration\nFor various reasons, the predicted probabilities from models do not always align with the observed frequencies of detections. For example, we would hope that if we look at all sites with a estimated probability of encounter of 0.2, that 20% of these would record the species. However, these probabilities are not always so well aligned. This will clearly be the case in our example, because we have deliberately inflated the prevalence of detection records in the data through the spatiotemporal subsampling process. We can produce a calibration model for the predictions, which can be a useful diagnostic tool to understand the model predictions, and in some cases can be used to realign the predictions with observations. For information on calibration in species distribution models see Vaughan and Ormerod (2005) and for more fundamental references on calibration see Platt (1999), Murphy (1973), and Niculescu-Mizil and Caruana (2005).\nTo train a calibration model for our predictions, we predict encounter rate for each checklist in the training set, then fit a binomial Generalized Additive Model (GAM) with the real observed encounter rate as the response and the predicted encounter rate as the predictor variable. Whereas GLMs fit a linear relationship between a response and predictors, GAMs allow non-linear relationships. Although GAMs provide a degree of flexibility, in some situations they may overfit and provide unrealistic and unhelpful calibrations. We have a strong a priori expectation that higher real values will also be associated with higher estimated encounter rates. In order to maintain the ranking of predictions, it is important that we respect this ordering and to do this we’ll use a GAM that is constrained to only increase. To fit the GAM, we’ll use the R package scam, so the shape can be constrained to be monotonically increasing. Note that predictions from ranger are in the form of a matrix of probabilities for each class, and we want the probability of detections, which is the second column of this matrix.\n\n# predicted encounter rate based on out of bag samples\ner_pred &lt;- er_model$predictions[, 2]\n# observed detection, converted back from factor\ndet_obs &lt;- as.integer(checklists_train$species_observed)\n# construct a data frame to train the scam model\nobs_pred &lt;- data.frame(obs = det_obs, pred = er_pred)\n\n# train calibration model\ncalibration_model &lt;- scam(obs ~ s(pred, k = 6, bs = \"mpi\"), \n                          gamma = 2,\n                          data = obs_pred)\n\nTo use the calibration model as a diagnostic tool, we’ll group the predicted encounter rates into bins, then calculate the mean predicted and observed encounter rates within each bin. This can be compared to predictions from the calibration model.\n\n# group the predicted encounter rate into bins of width 0.02\n# then calculate the mean observed encounter rates in each bin\ner_breaks &lt;- seq(0, 1, by = 0.02)\nmean_er &lt;- obs_pred |&gt;\n  mutate(er_bin = cut(pred, breaks = er_breaks, include.lowest = TRUE)) |&gt;\n  group_by(er_bin) |&gt;\n  summarise(n_checklists = n(),\n            pred = mean(pred), \n            obs = mean(obs),\n            .groups = \"drop\")\n\n# make predictions from the calibration model\ncalibration_curve &lt;- data.frame(pred = er_breaks)\ncal_pred &lt;- predict(calibration_model, calibration_curve, type = \"response\")\ncalibration_curve$calibrated &lt;- cal_pred\n\n# compared binned mean encounter rates to calibration model\nggplot(calibration_curve) +\n  aes(x = pred, y = calibrated) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  geom_line(color = \"blue\") +\n  geom_point(data = mean_er, \n             aes(x = pred, y = obs),\n             size = 2, alpha = 0.6,\n             show.legend = FALSE) +\n  labs(x = \"Estimated encounter rate\",\n       y = \"Observed encounter rate\",\n       title = \"Calibration model\") +\n  coord_equal(xlim = c(0, 1), ylim = c(0, 1))\n\n\n\n\n\n\n\n\nFrom this plot we can clearly see that the estimated encounter rates are mostly much larger than the observed encounter rates (all points fall below the dashed \\(x = y\\) line). So we see that the model is not well calibrated. However, we do see from the points that the relative ranking of predictions is largely good: sites with estimated higher encounter rate do mostly have higher observed encounter rates.\nFrom this we have learnt that the model is good at distinguishing sites with high rates from those with low rates. For those readers familiar with using AUC scores to assess the quality of species distribution models, the graph is telling us that the model should have a high AUC value. However, the model is not so good at estimating encounter rates accurately.\nIf accurate encounter rates are required, and the calibration model is strong (close fit of points to the line in the figure above), then the calibration model can be used to calibrate the estimates from the random forest model, so they are adjusted to match the observed encounter rates more closely. The calibrated random forest model is the combination of the original random forest model followed by the calibration model.\nIf you’re using this model to calibrate your estimates, notice that the calibration curve can produce probabilities greater than 1 and less than 0, so when applying the calibration we also need to restrict the predictions to be between 0 and 1. It’s possible to run a logistic regression for the calibration to remove these predictions less than 0 or greater than 1; however, we’ve found the Gaussian constrained GAM to be more stable than the logistic constrained GAM.\n\n\n4.4.2 Thresholding\nThe random forest model produces continuous estimates of encounter rate from 0-1. However, for many applications, including assessing model performance, we’ll need to reclassify this continuous probability to a binary presence/absence estimate. This reclassification is done by setting a threshold above which the species is predicted to be present. The threshold is typically chosen to maximize a performance metric such as the Kappa statistic or the area under the ROC curve. However, for class imbalanced data, such as eBird data where non-detections are much more common, many of these metrics can inflate performance by over-weighting the more common class (Cao, Chicco, and Hoffman 2020). To mitigate these issues, we suggest a threshold setting method using the MCC-F1 curve. This method plots Matthews correlation coefficient (MCC) against the F1 score for a range of possible thresholds, then chooses the threshold where the curve is closest to the point of perfect performance. The R package mccf1 implements this method.\n\n# mcc and fscore calculation for various thresholds\nmcc_f1 &lt;- mccf1(\n  # observed detection/non-detection\n  response = obs_pred$obs,\n  # predicted encounter rate from random forest\n  predictor = obs_pred$pred)\n\n# identify best threshold\nmcc_f1_summary &lt;- summary(mcc_f1)\n#&gt;  mccf1_metric best_threshold\n#&gt;         0.399          0.508\nthreshold &lt;- mcc_f1_summary$best_threshold[1]\n\n\n\n\n\n\n\nTip\n\n\n\nThis threshold essentially defines the range boundary of the species: areas where encounter rate is below the threshold are predicted to be outside the range of Wood Thrush and areas where the encounter rate is above the threshold are predicted to be within range.\n\n\n\n\n4.4.3 Assessment\nTo assess the quality of the calibrated random forest model, we’ll validate the model’s ability to predict the observed patterns of detection using independent validation data (i.e. the 20% test data set). We’ll use a range of predictive performance metrics (PPMs) to compare the predictions to the actual observations. A majority of the metrics measure the ability of the model to correctly predict binary detection/non-detection, including: sensitivity, specificity, Precision-Recall AUC, F1 score, and MCC. Mean squared error (MSE) applies to the calibrated encounter rate estimates.\nTo ensure bias in the test data set doesn’t impact the predictive performance metrics, it’s important that we apply spatiotemporal grid sampling to the test data just as we did to the training data. We already performed this grid sampling above when we created the checklist_ss data frame, so we use that data frame here to calculate the PPMs.\n\n# get the test set held out from training\nchecklists_test &lt;- filter(checklists_ss, type == \"test\") |&gt; \n  mutate(species_observed = as.integer(species_observed))\n\n# predict to test data using random forest model\npred_er &lt;- predict(er_model, data = checklists_test, type = \"response\")\n# extract probability of detection\npred_er &lt;- pred_er$predictions[, 2]\n# convert predictions to binary (presence/absence) using the threshold\npred_binary &lt;- as.integer(pred_er &gt; threshold)\n# calibrate\npred_calibrated &lt;- predict(calibration_model, \n                           newdata = data.frame(pred = pred_er), \n                           type = \"response\") |&gt; \n  as.numeric()\n# constrain probabilities to 0-1\npred_calibrated[pred_calibrated &lt; 0] &lt;- 0\npred_calibrated[pred_calibrated &gt; 1] &lt;- 1\n# combine observations and estimates\nobs_pred_test &lt;- data.frame(id = seq_along(pred_calibrated),\n                            # actual detection/non-detection\n                            obs = as.integer(checklists_test$species_observed),\n                            # binary detection/on-detection prediction\n                            pred_binary = pred_binary,\n                            # calibrated encounter rate\n                            pred_calibrated = pred_calibrated)\n\n# mean squared error (mse)\nmse &lt;- mean((obs_pred_test$obs - obs_pred_test$pred_calibrated)^2, na.rm = TRUE)\n\n# precision-recall auc\nem &lt;- precrec::evalmod(scores = obs_pred_test$pred_binary, \n                       labels = obs_pred_test$obs)\npr_auc &lt;- precrec::auc(em) |&gt; \n  filter(curvetypes == \"PRC\") |&gt; \n  pull(aucs)\n\n# calculate metrics for binary prediction: sensitivity, specificity\npa_metrics &lt;- obs_pred_test |&gt; \n  select(id, obs, pred_binary) |&gt; \n  PresenceAbsence::presence.absence.accuracy(na.rm = TRUE, st.dev = FALSE)\n\n# mcc and f1\nmcc_f1 &lt;- calculate_mcc_f1(obs_pred_test$obs, obs_pred_test$pred_binary)\n\n# combine ppms together\nppms &lt;- data.frame(\n  mse = mse,\n  sensitivity = pa_metrics$sensitivity,\n  specificity = pa_metrics$specificity,\n  pr_auc = pr_auc,\n  mcc = mcc_f1$mcc,\n  f1 = mcc_f1$f1\n)\nknitr::kable(pivot_longer(ppms, everything()), digits = 3)\n\n\n\n\nname\nvalue\n\n\n\n\nmse\n0.086\n\n\nsensitivity\n0.700\n\n\nspecificity\n0.804\n\n\npr_auc\n0.291\n\n\nmcc\n0.380\n\n\nf1\n0.454\n\n\n\n\n\nAn important aspect of eBird data to remember is that it is heavily imbalanced, with many more non-detections than detections, and this has an impact on the interpretation of predictive performance metrics that incorporate the true negative rate. Accordingly, it is more informative to look at the precision-recall (PR) AUC compared to the ROC AUC because neither precision nor recall incorporate the true negative rate. Each metric provides information about different aspects of the model fit and as each scales from 0-1 they provide a relatively standardized way of comparing model fits across species, regions, and seasons."
  },
  {
    "objectID": "encounter.html#sec-encounter-habitat",
    "href": "encounter.html#sec-encounter-habitat",
    "title": "4  Encounter Rate",
    "section": "4.5 Habitat associations",
    "text": "4.5 Habitat associations\nFrom the random forest model, we can glean two important sources of information about the association between Wood Thrush detection and features of their local environment. First, predictor importance is a measure of the predictive power of each variable used as a predictor in the model, and is calculated as a byproduct of fitting a random forests model. Second, partial dependence estimates the marginal effect of one predictor holding all other predictors constant.\n\n4.5.1 Predictor importance\nDuring the process of training a random forests model, some variables are removed at each node of the trees that make up the random forests. Predictor importance is based on the mean decrease in accuracy of the model when a given predictor is not used. It’s technically an average Gini index, but essentially larger values indicate that a predictor is more important to the model.\n\n# extract predictor importance from the random forest model object\npred_imp &lt;- er_model$variable.importance\npred_imp &lt;- data.frame(predictor = names(pred_imp), \n                       importance = pred_imp) |&gt; \n  arrange(desc(importance))\n# plot importance of top 20 predictors\nggplot(head(pred_imp, 20)) + \n  aes(x = reorder(predictor, importance), y = importance) +\n  geom_col() +\n  geom_hline(yintercept = 0, linewidth = 2, colour = \"#555555\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  coord_flip() +\n  labs(x = NULL, \n       y = \"Predictor Importance (Gini Index)\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        panel.grid.major.x = element_line(colour = \"#cccccc\", linewidth = 0.5))\n\n\n\n\n\n\n\n\nThe most important predictors of detection/non-detection are often effort variables. Indeed, that’s the case here: checklist duration, distance traveled, and start time (hours_of_day) all appear in the top 5 predictors. This is not surprising: going out at the right time of day and expending more effort searching will lead to a higher probability of detecting Wood Thrush. Focusing on the habitat variables, both elevation variables have high importance, and the top habitat variables are from deciduous broadleaf forest and woody savanna. Note however, that high importance doesn’t tell us the direction of the relationship with detection, for that we’ll have to look at partial dependence plots.\n\n\n4.5.2 Partial dependence\nPartial dependence plots show the marginal effect of a given predictor on encounter rate averaged across the other predictors. These plots are generated by predicting encounter rate at a regular sequence of points across the full range of values of a given predictor. At each predictor value, predictions of encounter rate are made for a random subsample of the training dataset with the focal predictor fixed, but all other predictors left as is. The encounter rate predictions are then averaged across all the checklists in the training dataset giving an estimate of the average encounter rate at a specific value of the focal predictor. This is a cumbersome process, but we provide a function below that does all the hard work for you! This function takes the following arguments:\n\npredictor: the name of the predictor to calculate partial dependence for\ner_model: the encounter rate model object\ncalibartion_model: the calibration model object\ndata: the original data used to train the model\nx_res: the resolution of the grid over which to calculate the partial dependence, i.e. the number of points between the minimum and maximum values of the predictor to evaluate partial dependence at\nn: number of points to subsample from the training data\n\n\n# function to calculate partial dependence for a single predictor\ncalculate_pd &lt;- function(predictor, er_model, calibration_model,\n                         data, x_res = 25, n = 1000) {\n  # create prediction grid using quantiles\n  x_grid &lt;- quantile(data[[predictor]],\n                     probs = seq(from = 0, to = 1, length = x_res),\n                     na.rm = TRUE)\n  # remove duplicates\n  x_grid &lt;- x_grid[!duplicated(signif(x_grid, 8))]\n  x_grid &lt;- unname(unique(x_grid))\n  grid &lt;- data.frame(predictor = predictor, x = x_grid)\n  names(grid) &lt;- c(\"predictor\", predictor)\n  \n  # subsample training data\n  n &lt;- min(n, nrow(data))\n  data &lt;- data[sample(seq.int(nrow(data)), size = n, replace = FALSE), ]\n  \n  # drop focal predictor from data\n  data &lt;- data[names(data) != predictor]\n  grid &lt;- merge(grid, data, all = TRUE)\n  \n  # predict encounter rate\n  p &lt;- predict(er_model, data = grid)\n  \n  # summarize\n  pd &lt;- grid[, c(\"predictor\", predictor)]\n  names(pd) &lt;- c(\"predictor\", \"x\")\n  pd$encounter_rate &lt;- p$predictions[, 2]\n  pd &lt;- dplyr::group_by(pd, predictor, x)\n  pd &lt;- dplyr::summarise(pd,\n                         encounter_rate = mean(encounter_rate, na.rm = TRUE),\n                         .groups = \"drop\")\n  \n  # calibrate\n  pd$encounter_rate &lt;- predict(calibration_model, \n                               newdata = data.frame(pred = pd$encounter_rate), \n                               type = \"response\")\n  pd$encounter_rate &lt;- as.numeric(pd$encounter_rate)\n  # constrain to 0-1\n  pd$encounter_rate[pd$encounter_rate &lt; 0] &lt;- 0\n  pd$encounter_rate[pd$encounter_rate &gt; 1] &lt;- 1\n\n  return(pd)\n}\n\nNow we’ll use this function to calculate partial dependence for the top 6 predictors.\n\n# calculate partial dependence for each of the top 6 predictors\npd &lt;- NULL\nfor (predictor in head(pred_imp$predictor)) {\n  pd &lt;- calculate_pd(predictor, \n                     er_model = er_model, \n                     calibration_model = calibration_model,\n                     data = checklists_train) |&gt; \n    bind_rows(pd)\n}\nhead(pd)\n#&gt; # A tibble: 6 × 3\n#&gt;   predictor                         x encounter_rate\n#&gt;   &lt;chr&gt;                         &lt;dbl&gt;          &lt;dbl&gt;\n#&gt; 1 pland_c04_deciduous_broadleaf  0            0.0955\n#&gt; 2 pland_c04_deciduous_broadleaf  2.33         0.0979\n#&gt; 3 pland_c04_deciduous_broadleaf  2.56         0.0999\n#&gt; 4 pland_c04_deciduous_broadleaf  4.88         0.101 \n#&gt; 5 pland_c04_deciduous_broadleaf  6.98         0.102 \n#&gt; 6 pland_c04_deciduous_broadleaf  9.52         0.102\n\n# plot partial dependence\nggplot(pd) +\n  aes(x = x, y = encounter_rate) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ factor(predictor, levels = rev(unique(predictor))), \n             ncol = 2, scales = \"free\") +\n  labs(x = NULL, y = \"Encounter Rate\") +\n  theme_minimal() +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        axis.line = element_line(color = \"grey60\"),\n        axis.ticks  = element_line(color = \"grey60\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\nConsider the relationships shown in the partial dependence plots in light of your knowledge of the species. Do these relationships make sense?\n\n\nThere are a range of interesting responses here. As seen in Section 2.9, the encounter rate for Wood Thrush peaks early in the morning when they’re most likely to be singing, then quickly drops off in the middle of the day, before slightly increasing in the evening. Some other predictors show a more smoothly increasing relationship with encounter rate, for example, as the landscape contains more deciduous forest, the encounter rate increases.\nThe random forest model has a number of interactions, which are not displayed in these partial dependence plots. When interpreting these, bear in mind that there are likely some more complex interaction effects beneath these individual plots.\n\n\n\n\n\n\nExercise\n\n\n\nExamine the predictor importance data to identify an the next most important percent landcover variables after pland_c04_deciduous_broadleaf. Make a partial dependence plot for this variable.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLooking at the predictor importance data frame, we can identify that pland_c08_woody_savanna is the next most important percent landcover variable.\n\n# calculate partial dependence\npd_woody_savanna &lt;- calculate_pd(\"pland_c08_woody_savanna\", \n                                 er_model = er_model, \n                                 calibration_model = calibration_model,\n                                 data = checklists_train)\n\n# plot partial dependence\nggplot(pd_woody_savanna) +\n  aes(x = x, y = encounter_rate) +\n  geom_line() +\n  geom_point() +\n  labs(x = NULL, y = \"Encounter Rate\") +\n  theme_minimal() +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        axis.line = element_line(color = \"grey60\"),\n        axis.ticks  = element_line(color = \"grey60\"))"
  },
  {
    "objectID": "encounter.html#sec-encounter-predict",
    "href": "encounter.html#sec-encounter-predict",
    "title": "4  Encounter Rate",
    "section": "4.6 Prediction",
    "text": "4.6 Prediction\nIn Section 3.4, we created a prediction grid consisting of the habitat variables summarized on a regular grid of points across the study region. In this section, we’ll make predictions of encounter rate at these points.\n\n4.6.1 Standardized effort variables\nThe prediction grid only includes values for the environmental variables, so to make predictions we’ll need to add effort variables to this prediction grid. We’ll make predictions for a standard eBird checklist: a 2 km, 1 hour traveling count at the peak time of day for detecting this species. Finally, we’ll make these predictions for June 15, 2023, the middle of our June focal window for the latest year for which we have eBird data.\nTo find the time of day with the highest detection probability, we can look for the peak of the partial dependence plot. The one caveat to this approach is that it’s important we focus on times of day for which there are enough data to make predictions. In particular, there’s an increasing trend in detectability with earlier start times, and few checklists late at night, which can cause the model to incorrectly extrapolate that trend to show highest detectability at night. Let’s start by looking at a plot to see if this is happening here.\n\n# find peak time of day from partial dependence\npd_time &lt;- calculate_pd(\"hours_of_day\",\n                        er_model = er_model, \n                        calibration_model = calibration_model,\n                        data = checklists_train) |&gt; \n  select(hours_of_day = x, encounter_rate)\n\n# histogram\ng_hist &lt;- ggplot(checklists_train) +\n  aes(x = hours_of_day) +\n  geom_histogram(binwidth = 1, center = 0.5, color = \"grey30\",\n                 fill = \"grey50\") +\n  scale_x_continuous(breaks = seq(0, 24, by = 3)) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(x = \"Hours since midnight\",\n       y = \"# checklists\",\n       title = \"Distribution of observation start times\")\n\n# partial dependence plot\ng_pd &lt;- ggplot(pd_time) +\n  aes(x = hours_of_day, y = encounter_rate) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(0, 24, by = 3)) +\n  labs(x = \"Hours since midnight\",\n       y = \"Encounter rate\",\n       title = \"Observation start time partial dependence\")\n\n# combine\ngrid.arrange(g_hist, g_pd)\n\n\n\n\n\n\n\n\nThe peak probability of reporting occurs early in the morning, as expected for a songbird like Wood Thrush. However, there is some odd behavior at the minimum and maximum values of hours_of_day where extrapolation is occurring due to a shortage of data. In general, trimming the first and last values, then selecting the value of hours_of_day that maximizes encounter rate is a reliable method of avoiding extrapolation.\n\n# trim ends of partial dependence\npd_time_trimmed &lt;- pd_time[c(-1, -nrow(pd_time)), ]\n\n# identify time maximizing encounter rate\npd_time_trimmed &lt;- arrange(pd_time_trimmed, desc(encounter_rate))\nt_peak &lt;- pd_time_trimmed$hours_of_day[1]\nprint(t_peak)\n#&gt; [1] 6.67\n\nBased on this analysis, the best time for detecting Wood Thrush is at 6:40 AM. We’ll use this time to make predictions. This is equivalent to many eBirders all conducting a checklist within different grid cells on June 15 at 6:40 AM. We also add the other effort variables to the prediction grid dataset at this time.\n\n# add effort covariates to prediction grid\npred_grid_eff &lt;- pred_grid |&gt; \n  mutate(observation_date = ymd(\"2023-06-15\"),\n         year = year(observation_date),\n         day_of_year = yday(observation_date),\n         hours_of_day = t_peak,\n         effort_distance_km = 2,\n         effort_hours = 1,\n         effort_speed_kmph = 2,\n         number_observers = 1)\n\n\n\n4.6.2 Model estimates\nUsing these standardized effort variables we can now make estimates across the prediction surface. We’ll use the model to estimate both encounter rate and binary detection/non-detection. The binary prediction, based on the MCC-F1 optimizing threshold we calculated in Section 4.4.2, acts as an estimate of the range boundary of Wood Thrush in Georgia in June.\n\n# estimate encounter rate\npred_er &lt;- predict(er_model, data = pred_grid_eff, type = \"response\")\npred_er &lt;- pred_er$predictions[, 2]\n# define range-boundary\npred_binary &lt;- as.integer(pred_er &gt; threshold)\n# apply calibration\npred_er_cal &lt;- predict(calibration_model, \n                       data.frame(pred = pred_er), \n                       type = \"response\") |&gt; \n  as.numeric()\n# constrain to 0-1\npred_er_cal[pred_er_cal &lt; 0] &lt;- 0\npred_er_cal[pred_er_cal &gt; 1] &lt;- 1\n# combine predictions with coordinates from prediction grid\npredictions &lt;- data.frame(cell_id = pred_grid_eff$cell_id,\n                          x = pred_grid_eff$x,\n                          y = pred_grid_eff$y,\n                          in_range = pred_binary, \n                          encounter_rate = pred_er_cal)\n\nNext, we’ll convert this data frame to spatial features using sf, then rasterize the points using the prediction grid raster template.\n\nr_pred &lt;- predictions |&gt; \n  # convert to spatial features\n  st_as_sf(coords = c(\"x\", \"y\"), crs = crs) |&gt; \n  select(in_range, encounter_rate) |&gt; \n  # rasterize\n  rasterize(r, field = c(\"in_range\", \"encounter_rate\"))\nprint(r_pred)\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 171, 148, 2  (nrow, ncol, nlyr)\n#&gt; resolution  : 3000, 3000  (x, y)\n#&gt; extent      : -175612, 268388, -312494, 200506  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=laea +lat_0=33.2 +lon_0=-83.7 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs \n#&gt; source(s)   : memory\n#&gt; names       : in_range, encounter_rate \n#&gt; min values  :        0,          0.000 \n#&gt; max values  :        1,          0.693"
  },
  {
    "objectID": "encounter.html#sec-encounter-map",
    "href": "encounter.html#sec-encounter-map",
    "title": "4  Encounter Rate",
    "section": "4.7 Mapping",
    "text": "4.7 Mapping\nNow for the fun part: let’s make a maps of the distribution of Wood Thrush in Georgia! We’ll start by making a simple range map using the in_range layer of the raster of predictions we created. Although the encounter rate predictions provide more detailed information, for some applications a range map will be more desirable.\n\npar(mar = c(0.25, 0.25, 1.25, 0.25))\n# set up plot area\nplot(study_region, \n     main = \"Wood Thrush Range (June 2023)\",\n     col = NA, border = NA)\nplot(ne_land, col = \"#cfcfcf\", border = \"#888888\", lwd = 0.5, add = TRUE)\n\n# convert binary prediction to categorical\nr_range &lt;- as.factor(r_pred[[\"in_range\"]])\nplot(r_range, col = c(\"#e6e6e6\", \"forestgreen\"),\n     maxpixels = ncell(r_range),\n     legend = FALSE, axes = FALSE, bty = \"n\",\n     add = TRUE)\n\n# borders\nplot(ne_state_lines, col = \"#ffffff\", lwd = 0.75, add = TRUE)\nplot(ne_country_lines, col = \"#ffffff\", lwd = 1.5, add = TRUE)\nplot(study_region, border = \"#000000\", col = NA, lwd = 1, add = TRUE)\nbox()\n\n\n\n\n\n\n\n\nNext we’ll make a map of the encounter rate predictions.\n\npar(mar = c(4, 0.25, 0.25, 0.25))\n# set up plot area\nplot(study_region, col = NA, border = NA)\nplot(ne_land, col = \"#cfcfcf\", border = \"#888888\", lwd = 0.5, add = TRUE)\n\n# define quantile breaks\nbrks &lt;- global(r_pred[[\"encounter_rate\"]], fun = quantile, \n               probs = seq(0, 1, 0.1), na.rm = TRUE) |&gt; \n  as.numeric() |&gt; \n  unique()\n# label the bottom, middle, and top value\nlbls &lt;- round(c(0, median(brks), max(brks)), 2)\n# ebird status and trends color palette\npal &lt;- ebirdst_palettes(length(brks) - 1)\nplot(r_pred[[\"encounter_rate\"]], \n     col = pal, breaks = brks, \n     maxpixels = ncell(r_pred),\n     legend = FALSE, axes = FALSE, bty = \"n\",\n     add = TRUE)\n\n# borders\nplot(ne_state_lines, col = \"#ffffff\", lwd = 0.75, add = TRUE)\nplot(ne_country_lines, col = \"#ffffff\", lwd = 1.5, add = TRUE)\nplot(study_region, border = \"#000000\", col = NA, lwd = 1, add = TRUE)\nbox()\n\n# legend\npar(new = TRUE, mar = c(0, 0, 0, 0))\ntitle &lt;- \"Wood Thrush Encounter Rate (June 2023)\"\nimage.plot(zlim = c(0, 1), legend.only = TRUE, \n           col = pal, breaks = seq(0, 1, length.out = length(brks)),\n           smallplot = c(0.25, 0.75, 0.03, 0.06),\n           horizontal = TRUE,\n           axis.args = list(at = c(0, 0.5, 1), labels = lbls,\n                            fg = \"black\", col.axis = \"black\",\n                            cex.axis = 0.75, lwd.ticks = 0.5,\n                            padj = -1.5),\n           legend.args = list(text = title,\n                              side = 3, col = \"black\",\n                              cex = 1, line = 0))\n\n\n\n\n\n\n\n\n\n\n\n\nCao, Chang, Davide Chicco, and Michael M. Hoffman. 2020. “The MCC-F1 Curve: A Performance Evaluation Technique for Binary Classification.” https://doi.org/10.48550/ARXIV.2006.11278.\n\n\nChen, Chao, Andy Liaw, and Leo Breiman. 2004. “Using Random Forest to Learn Imbalanced Data.” University of California, Berkeley 110 (1-12): 24.\n\n\nGuillera-Arroita, Gurutzeta, José J. Lahoz-Monfort, Jane Elith, Ascelin Gordon, Heini Kujala, Pia E. Lentini, Michael A. McCarthy, Reid Tingley, and Brendan A. Wintle. 2015. “Is My Species Distribution Model Fit for Purpose? Matching Data and Models to Applications.” Global Ecology and Biogeography 24 (3): 276–92.\n\n\nMurphy, Allan H. 1973. “A New Vector Partition of the Probability Score.” Journal of Applied Meteorology 12 (4): 595–600. https://doi.org/10.1175/1520-0450(1973)012&lt;0595:ANVPOT&gt;2.0.CO;2.\n\n\nNiculescu-Mizil, Alexandru, and Rich Caruana. 2005. “Predicting Good Probabilities with Supervised Learning.” In Proceedings of the 22nd International Conference on Machine Learning, 625–32. ACM.\n\n\nPlatt, John. 1999. “Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods.” Advances in Large Margin Classifiers 10 (3): 61–74.\n\n\nRobinson, Orin J., Viviana Ruiz-Gutierrez, and Daniel Fink. 2018. “Correcting for Bias in Distribution Modelling for Rare Species Using Citizen Science Data.” Diversity and Distributions 24 (4): 460–72. https://doi.org/10.1111/ddi.12698.\n\n\nVaughan, I. P., and S. J. Ormerod. 2005. “The Continuing Challenges of Testing Species Distribution Models.” Journal of Applied Ecology 42 (4): 720–30. https://doi.org/10.1111/j.1365-2664.2005.01052.x."
  },
  {
    "objectID": "abundance.html#sec-encounter-data",
    "href": "abundance.html#sec-encounter-data",
    "title": "5  Relative Abundance",
    "section": "5.1 Data preparation",
    "text": "5.1 Data preparation\nLet’s get started by loading the necessary packages and data. If you worked through the previous chapters, you should have all the data required for this chapter. However, you may want to download the data package, and unzip it to your project directory, to ensure you’re working with exactly the same data as was used in the creation of this guide.\n\nlibrary(dplyr)\nlibrary(ebirdst)\nlibrary(fields)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(lubridate)\nlibrary(mccf1)\nlibrary(ranger)\nlibrary(readr)\nlibrary(scam)\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyr)\n\n# set random number seed for reproducibility\nset.seed(1)\n\n# environmental variables: landcover and elevation\nenv_vars &lt;- read_csv(\"data/environmental-variables_checklists_jun_us-ga.csv\")\n\n# zero-filled ebird data combined with environmental data\nchecklists &lt;- read_csv(\"data/checklists-zf_woothr_jun_us-ga.csv\") |&gt; \n  inner_join(env_vars, by = \"checklist_id\")\n\n# prediction grid\npred_grid &lt;- read_csv(\"data/environmental-variables_prediction-grid_us-ga.csv\")\n# raster template for the grid\nr &lt;- rast(\"data/prediction-grid_us-ga.tif\")\n# get the coordinate reference system of the prediction grid\ncrs &lt;- st_crs(r)\n\n# load gis data for making maps\nstudy_region &lt;- read_sf(\"data/gis-data.gpkg\", \"ne_states\") |&gt; \n  filter(state_code == \"US-GA\") |&gt; \n  st_transform(crs = crs) |&gt; \n  st_geometry()\nne_land &lt;- read_sf(\"data/gis-data.gpkg\", \"ne_land\") |&gt; \n  st_transform(crs = crs) |&gt; \n  st_geometry()\nne_country_lines &lt;- read_sf(\"data/gis-data.gpkg\", \"ne_country_lines\") |&gt; \n  st_transform(crs = crs) |&gt; \n  st_geometry()\nne_state_lines &lt;- read_sf(\"data/gis-data.gpkg\", \"ne_state_lines\") |&gt; \n  st_transform(crs = crs) |&gt; \n  st_geometry()\n\nNext, following the approach outlined in Section 4.3, we’ll perform a round of spatiotemporal subsampling on the data to reduce bias.\n\n# sample one checklist per 3km x 3km x 1 week grid for each year\n# sample detection/non-detection independently \nchecklists_ss &lt;- grid_sample_stratified(checklists,\n                                        obs_column = \"species_observed\",\n                                        sample_by = \"type\")\n\nFinally, we’ll remove the 20% of checklists held aside for testing and select only the columns we intend to use as predictors to train the models.\n\nchecklists_train &lt;- checklists_ss |&gt; \n  filter(type == \"train\") |&gt; \n  # select only the columns to be used in the model\n  select(species_observed, observation_count,\n         year, day_of_year, hours_of_day,\n         effort_hours, effort_distance_km, effort_speed_kmph,\n         number_observers, \n         starts_with(\"pland_\"),\n         starts_with(\"ed_\"),\n         starts_with(\"elevation_\"))"
  },
  {
    "objectID": "abundance.html#sec-abundance-hurdle",
    "href": "abundance.html#sec-abundance-hurdle",
    "title": "5  Relative Abundance",
    "section": "5.2 Hurdle model",
    "text": "5.2 Hurdle model\nFor this two-step hurdle model, we’ll start by training exactly the same encounter rate model as in the Chapter 4. Then we’ll subset the eBird checklist to only those where the species was detected or predicted to occur by the encounter rate model. We’ll use this subset of the data to train a second random forests model for expected count. Finally we’ll combine the results of the two steps together to produce estimates of relative abundance.\n\n5.2.1 Step 1: Encounter rate\nIf you haven’t done so, read Chapter 4 for details on the calibrated encounter rate model. Here we repeat the process of modeling encounter rate in a compressed form.\n\n# calculate detection frequency for the balance random forest\ndetection_freq &lt;- mean(checklists_train$species_observed)\n\n# train a random forest model for encounter rate\ntrain_er &lt;- select(checklists_train, -observation_count)\ner_model &lt;- ranger(formula =  as.factor(species_observed) ~ ., \n                   data = train_er,\n                   importance = \"impurity\",\n                   probability = TRUE,\n                   replace = TRUE,\n                   sample.fraction = c(detection_freq, detection_freq))\n\n# select the mcc-f1 optimizing occurrence threshold\nobs_pred &lt;- data.frame(obs = as.integer(train_er$species_observed), \n                       pred = er_model$predictions[, 2])\nmcc_f1 &lt;- mccf1(response = obs_pred$obs, predictor = obs_pred$pred)\nmcc_f1_summary &lt;- summary(mcc_f1)\nthreshold &lt;- mcc_f1_summary$best_threshold[1]\n\n# calibration model\ncalibration_model &lt;- scam(obs ~ s(pred, k = 6, bs = \"mpi\"), \n                          gamma = 2,\n                          data = obs_pred)\n#&gt;  mccf1_metric best_threshold\n#&gt;         0.397          0.585\n\n\n\n5.2.2 Step 2: Count\nFor the second step, we train a random forests model to estimate the expected count of individuals on eBird checklists where the species was detected or predicted to be detected by the encounter rate model. So, we’ll start by subsetting the data to just these checklists. In addition, we’ll remove any observations for which the observer reported that Wood Thrush was present, but didn’t report a count of the number of individuals (coded as a count of “X” in the eBird database, but converted to NA in our dataset).\n\n# attach the predicted encounter rate based on out of bag samples\ntrain_count &lt;- checklists_train\ntrain_count$pred_er &lt;- er_model$predictions[, 2]\n# subset to only observed or predicted detections\ntrain_count &lt;- train_count |&gt; \n  filter(!is.na(observation_count),\n         observation_count &gt; 0 | pred_er &gt; threshold) |&gt; \n  select(-species_observed, -pred_er)\n\nWe’ve found that including estimated encounter rate as a predictor in the count model improves predictive performance. So, with this in mind, we predict encounter rate for the training dataset and add it as an additional column.\n\npredicted_er &lt;- predict(er_model, data = train_count, type = \"response\")\npredicted_er &lt;- predicted_er$predictions[, 2]\ntrain_count$predicted_er &lt;- predicted_er\n\nFinally, we train a random forests model to estimate count. This is superficially very similar to the random forests model for encounter rate; however, for count we’re using a regression random forest while for encounter rate we used a balanced classification random forest.\n\ncount_model &lt;- ranger(formula = observation_count ~ .,\n                      data = train_count,\n                      importance = \"impurity\",\n                      replace = TRUE)\n\n\n\n5.2.3 Assessment\nIn the Section 4.4.3 we calculated a suite of predictive performance metrics for the encounter rate model. These metrics should also be considered when modeling relative abundance; however, we won’t duplicate calculation of these metrics here. Instead we’ll calculate Spearman’s rank correlation coefficient for both count and relative abundance and Pearson correlation coefficient for the log of count and relative abundance. We’ll start by estimating encounter rate, count, and relative abundance for the spatiotemporally grid sampled test dataset.\n\n# get the test set held out from training\nchecklists_test &lt;- filter(checklists_ss, type == \"test\") |&gt; \n  mutate(species_observed = as.integer(species_observed)) |&gt; \n  filter(!is.na(observation_count))\n\n# estimate encounter rate for test data\npred_er &lt;- predict(er_model, data = checklists_test, type = \"response\")\n# extract probability of detection\npred_er &lt;- pred_er$predictions[, 2]\n# convert to binary using the threshold\npred_binary &lt;- as.integer(pred_er &gt; threshold)\n# calibrate\npred_calibrated &lt;- predict(calibration_model, \n                           newdata = data.frame(pred = pred_er), \n                           type = \"response\") |&gt; \n  as.numeric()\n# constrain probabilities to 0-1\npred_calibrated[pred_calibrated &lt; 0] &lt;- 0\npred_calibrated[pred_calibrated &gt; 1] &lt;- 1\n\n# add predicted encounter rate required for count estimates\nchecklists_test$predicted_er &lt;- pred_er\n# estimate count\npred_count &lt;- predict(count_model, data = checklists_test, type = \"response\")\npred_count &lt;- pred_count$predictions\n\n# relative abundance is the product of encounter rate and count\npred_abundance &lt;- pred_calibrated * pred_count\n\n# combine observations and estimates\nobs_pred_test &lt;- data.frame(\n  id = seq_along(pred_abundance),\n  # actual detection/non-detection\n  obs_detected = as.integer(checklists_test$species_observed),\n  obs_count = checklists_test$observation_count,\n  # model estimates\n  pred_binary = pred_binary,\n  pred_er = pred_calibrated,\n  pred_count = pred_count,\n  pred_abundance = pred_abundance\n)\n\nThe count metrics are measures of within range performance, meaning we compare observed count vs. estimated count only for those checklists where the model predicts the species to occur. Relative abundance accounts for both encounter rate and count, so the abundance predictive performance are based on all checklists.\n\n# subset to only those checklists where detect occurred\ndetections_test &lt;- filter(obs_pred_test, obs_detected &gt; 0)\n\n# count metrics, based only on checklists where detect occurred\ncount_spearman &lt;- cor(detections_test$pred_count, \n                      detections_test$obs_count,\n                      method = \"spearman\")\nlog_count_pearson &lt;- cor(log(detections_test$pred_count + 1),\n                         log(detections_test$obs_count + 1),\n                         method = \"pearson\")\n\n# abundance metrics\nabundance_spearman &lt;- cor(detections_test$pred_abundance, \n                          detections_test$obs_count,\n                          method = \"spearman\")\nlog_abundance_pearson &lt;- cor(log(detections_test$pred_abundance + 1),\n                             log(detections_test$obs_count + 1),\n                             method = \"pearson\")\n\n# combine ppms together\nppms &lt;- data.frame(\n  count_spearman = count_spearman,\n  log_count_pearson = log_count_pearson,\n  abundance_spearman = abundance_spearman,\n  log_abundance_pearson = log_abundance_pearson\n)\nknitr::kable(pivot_longer(ppms, everything()), digits = 3)\n\n\n\n\nname\nvalue\n\n\n\n\ncount_spearman\n0.225\n\n\nlog_count_pearson\n0.290\n\n\nabundance_spearman\n0.309\n\n\nlog_abundance_pearson\n0.405\n\n\n\n\n\nThe Spearman’s correlations tell us about the ability of the model to estimate the rank order of counts and relative abundance, something that these models often perform better with. The Pearson’s correlations give us information about the ability of the model to estimate absolute counts on the log scale, a task that is often more difficult to do with eBird data, especially for congregatory species that often have high counts. Again, as with the encounter rate performance metrics, these are useful in comparing model quality across species, region, and season."
  },
  {
    "objectID": "abundance.html#sec-abundance-predict",
    "href": "abundance.html#sec-abundance-predict",
    "title": "5  Relative Abundance",
    "section": "5.3 Prediction",
    "text": "5.3 Prediction\nJust as we did in the Section 4.6 for encounter rate, we can estimate relative abundance over our prediction grid. First we estimate encounter rate and count, then we multiply these together to get an estimate of relative abundance. Let’s start by adding the effort variables to the prediction grid for a standard eBird checklist at the optimal time of day for detecting Wood Thrush. Recall from the Section 4.6.1 that we determined the optimal time of day for detecting Wood Thrush was around 6:37AM.\n\npred_grid_eff &lt;- pred_grid |&gt; \n  mutate(observation_date = ymd(\"2023-06-15\"),\n         year = year(observation_date),\n         day_of_year = yday(observation_date),\n         # determined as optimal time for detection in previous chapter\n         hours_of_day = 6.6,\n         effort_distance_km = 2,\n         effort_hours = 1,\n         effort_speed_kmph = 2,\n         number_observers = 1)\n\nNow we can estimate calibrated encounter rate and count for each point on the prediction grid. We also include a binary estimate of the range boundary.\n\n# estimate encounter rate\npred_er &lt;- predict(er_model, data = pred_grid_eff, type = \"response\")\npred_er &lt;- pred_er$predictions[, 2]\n# define range-boundary\npred_binary &lt;- as.integer(pred_er &gt; threshold)\n# apply calibration\npred_er_cal &lt;- predict(calibration_model, \n                       data.frame(pred = pred_er), \n                       type = \"response\") |&gt; \n  as.numeric()\n# constrain to 0-1\npred_er_cal[pred_er_cal &lt; 0] &lt;- 0\npred_er_cal[pred_er_cal &gt; 1] &lt;- 1\n\n# add predicted encounter rate required for count estimates\npred_grid_eff$predicted_er &lt;- pred_er\n# estimate count\npred_count &lt;- predict(count_model, data = pred_grid_eff, type = \"response\")\npred_count &lt;- pred_count$predictions\n\n# combine predictions with coordinates from prediction grid\npredictions &lt;- data.frame(cell_id = pred_grid_eff$cell_id,\n                          x = pred_grid_eff$x,\n                          y = pred_grid_eff$y,\n                          in_range = pred_binary, \n                          encounter_rate = pred_er_cal,\n                          count = pred_count)\n\nNext, we add a column for the relative abundance estimate (the product of the encounter rate and count estimates), and convert these estimates to raster format.\n\n# add relative abundance estimate\npredictions$abundance &lt;- predictions$encounter_rate * predictions$count\n\n# rasterize\nlayers &lt;- c(\"in_range\", \"encounter_rate\", \"count\", \"abundance\")\nr_pred &lt;- predictions |&gt; \n  # convert to spatial features\n  st_as_sf(coords = c(\"x\", \"y\"), crs = crs) |&gt; \n  select(all_of(layers)) |&gt; \n  # rasterize\n  rasterize(r, field = layers)\nprint(r_pred)\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 171, 148, 4  (nrow, ncol, nlyr)\n#&gt; resolution  : 3000, 3000  (x, y)\n#&gt; extent      : -175612, 268388, -312494, 200506  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=laea +lat_0=33.2 +lon_0=-83.7 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs \n#&gt; source(s)   : memory\n#&gt; names       : in_range, encounter_rate, count, abundance \n#&gt; min values  :        0,          0.000, 0.133,      0.00 \n#&gt; max values  :        1,          0.715, 1.944,      1.22\n\nFinally we’ll produce a map of relative abundance. The values shown on this map are the expected number of Wood Thrush seen by an average eBirder conducting a 2 km, 1 hour traveling count starting at about 6:37AM on June 15, 2023. Since detectability is not perfect, we expect true Wood Thrush abundance to be higher than these values, but without estimating the detection rate directly it’s difficult to say how much higher.\nPrior to mapping the relative abundance, we’ll multiple by the in_range layer, which will produce a map showing zero relative abundance where the model predicts that Wood Thrush does not occur.\n\npar(mar = c(4, 0.25, 0.25, 0.25))\n# set up plot area\nplot(study_region, col = NA, border = NA)\nplot(ne_land, col = \"#cfcfcf\", border = \"#888888\", lwd = 0.5, add = TRUE)\n\n# define quantile breaks, excluding zeros\nbrks &lt;- ifel(r_pred[[\"abundance\"]] &gt; 0, r_pred[[\"abundance\"]], NA) |&gt; \n  global(fun = quantile, \n         probs = seq(0, 1, 0.1), na.rm = TRUE) |&gt; \n  as.numeric() |&gt; \n  unique()\n# label the bottom, middle, and top value\nlbls &lt;- round(c(min(brks), median(brks), max(brks)), 2)\n# ebird status and trends color palette\npal &lt;- ebirdst_palettes(length(brks) - 1)\nplot(r_pred[[\"abundance\"]], \n     col = c(\"#e6e6e6\", pal), breaks = c(0, brks), \n     maxpixels = ncell(r_pred),\n     legend = FALSE, axes = FALSE, bty = \"n\",\n     add = TRUE)\n\n# borders\nplot(ne_state_lines, col = \"#ffffff\", lwd = 0.75, add = TRUE)\nplot(ne_country_lines, col = \"#ffffff\", lwd = 1.5, add = TRUE)\nplot(study_region, border = \"#000000\", col = NA, lwd = 1, add = TRUE)\nbox()\n\n# legend\npar(new = TRUE, mar = c(0, 0, 0, 0))\ntitle &lt;- \"Wood Thrush Relative Abundance (June 2023)\"\nimage.plot(zlim = c(0, 1), legend.only = TRUE, \n           col = pal, breaks = seq(0, 1, length.out = length(brks)),\n           smallplot = c(0.25, 0.75, 0.03, 0.06),\n           horizontal = TRUE,\n           axis.args = list(at = c(0, 0.5, 1), labels = lbls,\n                            fg = \"black\", col.axis = \"black\",\n                            cex.axis = 0.75, lwd.ticks = 0.5,\n                            padj = -1.5),\n           legend.args = list(text = title,\n                              side = 3, col = \"black\",\n                              cex = 1, line = 0))\n\n\n\n\n\n\n\n\n\n\n\n\nKeyser, Spencer R., Daniel Fink, David Gudex-Cross, Volker C. Radeloff, Jonathan N. Pauli, and Benjamin Zuckerberg. 2023. “Snow Cover Dynamics: An Overlooked yet Important Feature of Winter Bird Occurrence and Abundance Across the United States.” Ecography 2023 (1). https://doi.org/10.1111/ecog.06378."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Amatulli, Giuseppe, Sami Domisch, Mao-Ning Tuanmu, Benoit Parmentier,\nAjay Ranipeta, Jeremy Malczyk, and Walter Jetz. 2018. “A Suite of\nGlobal, Cross-Scale Topographic Variables for Environmental and\nBiodiversity Modeling.” Scientific Data 5 (March):\n180040. https://doi.org/10.1038/sdata.2018.40.\n\n\nCao, Chang, Davide Chicco, and Michael M. Hoffman. 2020. “The\nMCC-F1 Curve: A Performance Evaluation Technique for Binary\nClassification.” https://doi.org/10.48550/ARXIV.2006.11278.\n\n\nChen, Chao, Andy Liaw, and Leo Breiman. 2004. “Using Random Forest\nto Learn Imbalanced Data.” University of California,\nBerkeley 110 (1-12): 24.\n\n\nCourter, Jason R., Ron J. Johnson, Claire M. Stuyck, Brian A. Lang, and\nEvan W. Kaiser. 2013. “Weekend Bias in Citizen\nScience Data Reporting: Implications for Phenology\nStudies.” International Journal of Biometeorology 57\n(5): 715–20. https://doi.org/10.1007/s00484-012-0598-7.\n\n\nEllis, Murray V., and Jennifer E. Taylor. 2018. “Effects of\nWeather, Time of Day, and Survey Effort on Estimates of Species Richness\nin Temperate Woodlands.” Emu-Austral Ornithology 118\n(2): 183–92.\n\n\nFriedl, Mark, and Damien Sulla-Menashe. 2015. “MCD12Q1\nMODIS/Terra+Aqua Land Cover Type Yearly L3\nGlobal 500m SIN Grid V006.” NASA EOSDIS\nLand Processes DAAC. https://doi.org/10.5067/MODIS/MCD12Q1.006.\n\n\nGreenwood, Jeremy J. D. 2007. “Citizens, Science and Bird\nConservation.” Journal of Ornithology 148 (1): 77–124.\nhttps://doi.org/10.1007/s10336-007-0239-9.\n\n\nGuillera-Arroita, Gurutzeta, José J. Lahoz-Monfort, Jane Elith, Ascelin\nGordon, Heini Kujala, Pia E. Lentini, Michael A. McCarthy, Reid Tingley,\nand Brendan A. Wintle. 2015. “Is My Species Distribution Model Fit\nfor Purpose? Matching Data and Models to\nApplications.” Global Ecology and Biogeography 24 (3):\n276–92.\n\n\nJohnston, Alison, Daniel Fink, Wesley M. Hochachka, and Steve Kelling.\n2018. “Estimates of Observer Expertise Improve Species\nDistributions from Citizen Science Data.” Methods in Ecology\nand Evolution 9 (1): 88–97.\n\n\nJohnston, Alison, Wesley M. Hochachka, Matthew E. Strimas-Mackey,\nViviana Ruiz Gutierrez, Orin J. Robinson, Eliot T. Miller, Tom Auer,\nSteve T. Kelling, and Daniel Fink. 2021. “Analytical Guidelines to\nIncrease the Value of Community Science Data: An Example\nUsing eBird Data to Estimate Species\nDistributions.” Edited by Yoan Fourcade. Diversity and\nDistributions 27 (7): 1265–77. https://doi.org/10.1111/ddi.13271.\n\n\nJohnston, Alison, Stuart E. Newson, Kate Risely, Andy J. Musgrove, Dario\nMassimino, Stephen R. Baillie, and James W. Pearce-Higgins. 2014.\n“Species Traits Explain Variation in Detectability of\nUK Birds.” Bird Study 61 (3): 340–50.\n\n\nKadmon, Ronen, Oren Farber, and Avinoam Danin. 2004. “Effect of\nRoadside Bias on the Accuracy of Predictive Maps Produced by Bioclimatic\nModels.” Ecological Applications 14 (2): 401–13.\n\n\nKelling, Steve, Alison Johnston, Daniel Fink, Viviana Ruiz-Gutierrez,\nRick Bonney, Aletta Bonn, Miguel Fernandez, et al. 2018. “Finding\nthe Signal in the Noise of Citizen Science\nObservations.” bioRxiv, May, 326314. https://doi.org/10.1101/326314.\n\n\nKeyser, Spencer R., Daniel Fink, David Gudex-Cross, Volker C. Radeloff,\nJonathan N. Pauli, and Benjamin Zuckerberg. 2023. “Snow Cover\nDynamics: An Overlooked yet Important Feature of Winter Bird Occurrence\nand Abundance Across the United States.”\nEcography 2023 (1). https://doi.org/10.1111/ecog.06378.\n\n\nLa Sorte, Frank A., Christopher A. Lepczyk, Jessica L. Burnett, Allen H.\nHurlbert, Morgan W. Tingley, and Benjamin Zuckerberg. 2018.\n“Opportunities and Challenges for Big Data Ornithology.”\nThe Condor 120 (2): 414–26.\n\n\nLuck, Gary W., Taylor H. Ricketts, Gretchen C. Daily, and Marc Imhoff.\n2004. “Alleviating Spatial Conflict Between People and\nBiodiversity.” Proceedings of the National Academy of\nSciences 101 (1): 182–86. https://doi.org/10.1073/pnas.2237148100.\n\n\nMurphy, Allan H. 1973. “A New Vector Partition of the\nProbability Score.” Journal of Applied\nMeteorology 12 (4): 595–600. https://doi.org/10.1175/1520-0450(1973)012&lt;0595:ANVPOT&gt;2.0.CO;2.\n\n\nNiculescu-Mizil, Alexandru, and Rich Caruana. 2005. “Predicting\nGood Probabilities with Supervised Learning.” In Proceedings\nof the 22nd International Conference on Machine\nLearning, 625–32. ACM.\n\n\nOliveira, Camilo Viana, Fabio Olmos, Manoel dos Santos-Filho, and\nChristine Steiner São Bernardo. 2018. “Observation of\nDiurnal Soaring Raptors In Northeastern Brazil Depends On Weather\nConditions and Time of Day.”\nJournal of Raptor Research 52 (1): 56–65.\n\n\nPlatt, John. 1999. “Probabilistic Outputs for Support Vector\nMachines and Comparisons to Regularized Likelihood Methods.”\nAdvances in Large Margin Classifiers 10 (3): 61–74.\n\n\nPrendergast, J. R., S. N. Wood, J. H. Lawton, and B. C. Eversham. 1993.\n“Correcting for Variation in Recording Effort in Analyses of\nDiversity Hotspots.” Biodiversity Letters, 39–53.\n\n\nR Core Team. 2023. R: A Language and Environment for\nStatistical Computing. Manual. Vienna, Austria:\nR Foundation for Statistical Computing.\n\n\nRobinson, Orin J., Viviana Ruiz-Gutierrez, and Daniel Fink. 2018.\n“Correcting for Bias in Distribution Modelling for Rare Species\nUsing Citizen Science Data.” Diversity and Distributions\n24 (4): 460–72. https://doi.org/10.1111/ddi.12698.\n\n\nRobinson, Orin J., Viviana Ruiz-Gutierrez, Daniel Fink, Robert J. Meese,\nMarcel Holyoak, and Evan G. Cooch. 2018. “Using Citizen Science\nData in Integrated Population Models to Inform Conservation\nDecision-Making.” bioRxiv, 293464.\n\n\nSullivan, Brian L., Jocelyn L. Aycrigg, Jessie H. Barry, Rick E. Bonney,\nNicholas Bruns, Caren B. Cooper, Theo Damoulas, et al. 2014. “The\neBird Enterprise: An Integrated\nApproach to Development and Application of Citizen Science.”\nBiological Conservation 169 (January): 31–40. https://doi.org/10.1016/j.biocon.2013.11.003.\n\n\nTulloch, Ayesha IT, and Judit K. Szabo. 2012. “A Behavioural\nEcology Approach to Understand Volunteer Surveying for Citizen Science\nDatasets.” Emu-Austral Ornithology 112 (4): 313–25.\n\n\nVaughan, I. P., and S. J. Ormerod. 2005. “The Continuing\nChallenges of Testing Species Distribution Models.” Journal\nof Applied Ecology 42 (4): 720–30. https://doi.org/10.1111/j.1365-2664.2005.01052.x."
  }
]